{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is a complex field which is hypothesised to be part of AI-complete set of problems, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem of making computers as intelligent as people. With over 90% of data ever generated being produced in the last 2 years and with a great proportion being human generated unstructured text there is an ever increasing need to advance the field of Natural Language Processing.\n",
    "\n",
    "Recent UK Government proposal to have measures to regulate social media companies over harmful content, including \"substantial\" fines and the ability to block services that do not stick to the rules is an example of the regulamentary need to better manage the content that is being generated by users.\n",
    "\n",
    "Other initiatives like ​Riot Games​' work aimed to predict and reform toxic player behaviour during games is another example of this effort to understand the content being generated by users and moderate toxic content.\n",
    "However, as highlighted by the Kaggle competition ​Jigsaw unintended bias in toxicity classification​, existing models suffer from unintended bias where models might predict high likelihood of toxicity for content containing certain words (e.g. \"gay\") even when those comments were not actually toxic (such as \"I am a gay woman\"), leaving machine only classification models still sub-standard.\n",
    "\n",
    "The outcome of our analysis is the type of algorithm that companies will use to define what is free speech and what shouldn't be tolerated in a discussion. This challenge actually starts with how the training dataset was produced: Multiple people (annotators) read thousands of comments and defined if those comments were offensive or not. Where is the trick? They disagreed in many of them. Having tools that are able to flag up toxic content without suffering from unintended bias is of paramount importance to preserve Internet's fairness and freedom of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n",
    "\n",
    "In the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment.\n",
    "\n",
    "For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning:\n",
      "\n",
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "\n",
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Documents\\Parth\\Data science\\Springboard\\github\\Springboard\\capstone projects\\2. jigsaw-unintended-bias-in-toxicity-classification\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import spacy\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "\n",
    "import watermark\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler,robust_scale,MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score\n",
    "\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "\n",
    "print(os.path.abspath(PROJ_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Library Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib 3.0.3\n",
      "scipy      1.2.1\n",
      "numpy      1.16.4\n",
      "gensim     3.4.0\n",
      "plotly     3.9.0\n",
      "watermark  1.8.1\n",
      "spacy      2.1.4\n",
      "seaborn    0.9.0\n",
      "nltk       3.4\n",
      "missingno  0.4.1\n",
      "re         2.2.1\n",
      "pandas     0.24.2\n",
      "Parth Patel 2019-07-21 14:20:10 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.4.0\n",
      "\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n"
     ]
    }
   ],
   "source": [
    "%watermark -a \"Parth Patel\" -d -t -v -p  numpy,pandas  --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "Since our main independat column is commets lets try to create features out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['total_length'] = train_df['comment_text'].apply(len)\n",
    "train_df['capitals'] = train_df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "train_df['caps_vs_length'] = train_df.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\n",
    "train_df['num_exclamation_marks'] = train_df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "train_df['num_question_marks'] = train_df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "train_df['num_punctuation'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "train_df['num_symbols'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "train_df['num_words'] = train_df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "train_df['num_unique_words'] = train_df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "train_df['words_vs_unique'] = train_df['num_unique_words'] / train_df['num_words']\n",
    "train_df['num_smilies'] = train_df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Is_toxic'] =  train_df['target'].apply(lambda x: \"Toxic\" if x>=0.5 else \"NonToxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks', 'num_punctuation', 'num_words', 'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nontoxic_df = train_df.loc[train_df['Is_toxic'] == 'NonToxic']\n",
    "Nontoxic_df = Nontoxic_df.head(481113)\n",
    "#Nontoxic_df = Nontoxic_df.head(30000)\n",
    "\n",
    "toxic_df = train_df.loc[train_df['Is_toxic'] == 'Toxic']\n",
    "#toxic_df = toxic_df.head(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625447"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del final_df\n",
    "final_df = pd.concat([Nontoxic_df,toxic_df])\n",
    "#Nontoxic_df.append(toxic_df) \n",
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1804874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonToxic    481113\n",
       "Toxic       144334\n",
       "Name: Is_toxic, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['Is_toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir()\n",
    "del Nontoxic_df\n",
    "del toxic_df\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625447"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['id','comment_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 625447 entries, 0 to 1804872\n",
      "Data columns (total 3 columns):\n",
      "id              625447 non-null int64\n",
      "comment_text    625447 non-null object\n",
      "target          625447 non-null float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Clening\n",
    "\n",
    "Text cleaning will be performed mainly in 5 steps, which will be \n",
    "* Lower caseing\n",
    "* Expanding Contractions\n",
    "* Removing Special Characters\n",
    "* Removing Stopwords\n",
    "\n",
    "#### Lower Case\n",
    "Here all alphabet will be converted to lower case as all avaible mapping are in lower case alphabet as well as it will remove inconsistant typing errors and make standard text words and sentence.\n",
    "\n",
    "#### Removing Special Characters\n",
    "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them.\n",
    "\n",
    "#### Removing Stopwords\n",
    "Words which have little or no significance, especially when constructing meaningful features from text, are known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are a, an, the, and the like.\n",
    "\n",
    "#### Lemmatization\n",
    "Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word, but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary. Both nltk and spacy have excellent lemmatizers. We will be using spacy here.\n",
    "\n",
    "#### Expanding Contractions\n",
    "Contractions are shortened version of words or syllables. They often exist in either written or spoken forms in the English language. These shortened versions or contractions of words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. Examples would be, do not to don’t and I would to I’d. Converting each contraction to its expanded, original form helps with text standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatizing words in dcoument\n",
    "    filtered_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['comment_text'] = normalize_corpus(final_df['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['comment_text'] = final_df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>cool like would want mother read really great ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank would make life lot le anxietyinducing k...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>urgent design problem kudos taking impressive</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>something ill able install site releasing</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59861</td>\n",
       "      <td>hahahahahahahahhha suck</td>\n",
       "      <td>0.457627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text    target\n",
       "0  59848  cool like would want mother read really great ...  0.000000\n",
       "1  59849  thank would make life lot le anxietyinducing k...  0.000000\n",
       "2  59852      urgent design problem kudos taking impressive  0.000000\n",
       "3  59855          something ill able install site releasing  0.000000\n",
       "6  59861                            hahahahahahahahhha suck  0.457627"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_df['comment_text']\n",
    "y=final_df['target']\n",
    "#final_df = final_df[['id','comment_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.apply(lambda x: 1 if x>=0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train_df[['comment_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500357,)\n",
      "(125090,)\n",
      "(500357,)\n",
      "(125090,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer(min_df = 0.01, ngram_range=(1, 2), analyzer=\"word\").fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trcv = cvect.transform(X_train)\n",
    "X_tscv = cvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125090, 417)\n",
      "(500357, 417)\n",
      "(500357,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tscv.shape)\n",
    "print(X_trcv.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvect = TfidfVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trtf = tfvect.transform(X_train)\n",
    "X_tstf = tfvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500357, 236688)\n",
      "(125090, 236688)\n",
      "(500357,)\n",
      "(125090,)\n"
     ]
    }
   ],
   "source": [
    "print(X_trtf.shape)\n",
    "print(X_tstf.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Count Vectoriser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas_alt = {'C': [0.001,0.01,0.1,1,10,100]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "alphas_alt = {'C': [0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5],\n",
    "            'max_iter' :[10,300,500]\n",
    "             }\n",
    "\n",
    "Logis = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  25 | elapsed:   37.4s remaining:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   40.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression Model : {'max_iter': 10, 'C': 1.4}\n"
     ]
    }
   ],
   "source": [
    "Logis = RandomizedSearchCV(estimator = Logis, param_distributions =alphas_alt, n_iter=5, cv = 5, verbose=2,n_jobs=-1, random_state=42)\n",
    "Logis.fit(X_trcv, y_train)\n",
    "\n",
    "print('Best parameters for Logistic Regression Model : {}'.format(Logis.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1300: UserWarning:\n",
      "\n",
      "'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.807149695117686\n",
      "Testing Accuracy: 0.8065872571748341\n"
     ]
    }
   ],
   "source": [
    "Logis_final = LogisticRegression(C=1.4,max_iter=10,n_jobs=-1)\n",
    "Logis_final.fit(X_trcv, y_train)\n",
    "#prediction = losso_final.predict(X_test)\n",
    "print(\"Training Accuracy: {}\".format(Logis_final.score(X_trcv, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(Logis_final.score(X_tscv, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.7970996374546818\n",
      "Test Recall: 0.2202646215497288\n",
      "F1 Score: 0.3451523845612516\n"
     ]
    }
   ],
   "source": [
    "predicted = Logis_final.predict(X_tscv)\n",
    "\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation :\n",
    "* Good initial model as Test and Train Accuracy are not very different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier\n",
    "We can see that the accuracy scores are not that bad. However, the precision and recall scores are just unacceptable. This is because of the imbalanced targets, as most of the targets have label 0 and few have label 1. So even a dumb classifier which always predicts the most common class would give a respectable accuracy score. So we need to compare our classifier's performance with once such most-common-class-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas_alt = {'C': [0.001,0.01,0.1,1,10,100]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "alphas_alt = {'strategy': [\"most_frequent\",\"stratified\",\"prior\",\"uniform\"]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "             }\n",
    "\n",
    "Dummy = DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:    0.8s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression Model : {'strategy': 'most_frequent'}\n"
     ]
    }
   ],
   "source": [
    "Dummy = RandomizedSearchCV(estimator = Dummy, param_distributions =alphas_alt, n_iter=4, cv = 5, verbose=2,n_jobs=-1, random_state=42)\n",
    "Dummy.fit(X_trcv, y_train)\n",
    "\n",
    "print('Best parameters for Logistic Regression Model : {}'.format(Dummy.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7693906550722784\n",
      "Testing Accuracy: 0.7685906147573747\n"
     ]
    }
   ],
   "source": [
    "#Logis_final = LogisticRegression(C=0.1,n_jobs=-1)\n",
    "Dummy_final = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "Dummy_final.fit(X_trcv, y_train)\n",
    "#prediction = losso_final.predict(X_test)\n",
    "print(\"Training Accuracy: {}\".format(Dummy_final.score(X_trcv, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(Dummy_final.score(X_tscv, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "predicted = Dummy_final.predict(X_tscv)\n",
    "\n",
    "\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Observations:\n",
    "* not a good model as basically it is predicting same thing for every input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance:\n",
    "Therefore, we see that our classifier is not much better than a simple baseline model which just predicts all outputs to be the most frequent class. Therefore, we need better models. Hence we will explore other models better suited for text classification purposes viz Naive Bayes Classifier and Support Vector Machines\n",
    "\n",
    "### Bernoulli Naive Bayes using Count Vectors as features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas_alt = {'C': [0.001,0.01,0.1,1,10,100]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "alphas_alt = {'alpha': [0.001,0.01,0.1,1,10,100]\n",
    "            \n",
    "             }\n",
    "\n",
    "Berrnoulli = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  25 | elapsed:    3.0s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression Model : {'alpha': 100}\n"
     ]
    }
   ],
   "source": [
    "Berrnoulli = RandomizedSearchCV(estimator = Berrnoulli, param_distributions =alphas_alt, n_iter=5, cv = 5, verbose=2,n_jobs=-1, random_state=42)\n",
    "Berrnoulli.fit(X_trcv, y_train)\n",
    "\n",
    "print('Best parameters for Logistic Regression Model : {}'.format(Berrnoulli.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8031385590688248\n",
      "Testing Accuracy: 0.8024222559756975\n"
     ]
    }
   ],
   "source": [
    "#Logis_final = LogisticRegression(C=0.1,n_jobs=-1)\n",
    "Berrnoulli_final = BernoulliNB(alpha=100)\n",
    "\n",
    "Berrnoulli_final.fit(X_trcv, y_train)\n",
    "#prediction = losso_final.predict(X_test)\n",
    "print(\"Training Accuracy: {}\".format(Berrnoulli_final.score(X_trcv, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(Berrnoulli_final.score(X_tscv, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.6489092188599578\n",
      "Test Recall: 0.31854769060697136\n",
      "F1 Score: 0.4273234932919341\n"
     ]
    }
   ],
   "source": [
    "predicted = Berrnoulli_final.predict(X_tscv)\n",
    "\n",
    "\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "We can see that the precision and recall scores have gone down, but the total accuracy score is almost same. Next we will run:\n",
    "\n",
    "### Multinomial NB with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas_alt = {'C': [0.001,0.01,0.1,1,10,100]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "#alphas_alt = {'alpha': [0.001,0.01,0.1,1,10,100]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "#alphas_alt = {'alpha': [0.05,0.08,0.1]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "\n",
    "alphas_alt = {'alpha': [0.01,0.02,0.03,0.04,0.05,0.07,0.08,0.09]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "             }\n",
    "Multinomial = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  25 | elapsed:    5.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression Model : {'alpha': 0.03}\n"
     ]
    }
   ],
   "source": [
    "Multinomial = RandomizedSearchCV(estimator = Multinomial, param_distributions =alphas_alt, n_iter=5, cv = 5, verbose=2,n_jobs=-1, random_state=42)\n",
    "Multinomial.fit(X_trtf, y_train)\n",
    "\n",
    "print('Best parameters for Logistic Regression Model : {}'.format(Multinomial.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8926946160441445\n",
      "Testing Accuracy: 0.8516747941482132\n"
     ]
    }
   ],
   "source": [
    "Multinomial_final = MultinomialNB(alpha=0.03)\n",
    "\n",
    "Multinomial_final.fit(X_trtf, y_train)\n",
    "#prediction = losso_final.predict(X_test)\n",
    "print(\"Training Accuracy: {}\".format(Multinomial_final.score(X_trtf, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(Multinomial_final.score(X_tstf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.8544921208813698\n",
      "Test Recall: 0.432721871005631\n",
      "F1 Score: 0.5745080952162547\n"
     ]
    }
   ],
   "source": [
    "predicted = Multinomial_final.predict(X_tstf)\n",
    "\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement:\n",
    "In this model the precision and accuracy have improved, however, the recall is very low. Lets now play around with SVM Models, which are also used very often in text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SVM Classifier with a linear kernel and TFIDF vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas_alt = {'C': [0.001,0.01,0.1,1,10,100],\n",
    "#             'kernel' :['rbf','linear'],\n",
    "#              'max_iter' :[1,5,10]\n",
    "#             }\n",
    "alphas_alt = {'C': [0.08,0.9,1,1.1,1.2,2],\n",
    "             'kernel' :['rbf','linear'],\n",
    "             'max_iter' :[80,100,300]\n",
    "             }\n",
    "#alphas_alt = {'alpha': [0.05,0.08,0.1]#,\n",
    "           #  'max_iter' :[100000,300000,500000]\n",
    "#             }\n",
    "\n",
    "\n",
    "SVCmodel = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  35 out of  35 | elapsed: 15.6min finished\n",
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning:\n",
      "\n",
      "Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression Model : {'max_iter': 100, 'kernel': 'rbf', 'C': 1.1}\n"
     ]
    }
   ],
   "source": [
    "SVCmodel = RandomizedSearchCV(estimator = SVCmodel, param_distributions =alphas_alt, n_iter=5, cv = 5, verbose=2,n_jobs=10, random_state=42)\n",
    "SVCmodel.fit(X_trtf, y_train)\n",
    "\n",
    "print('Best parameters for Logistic Regression Model : {}'.format(SVCmodel.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning:\n",
      "\n",
      "Solver terminated early (max_iter=80).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7847596815873467\n",
      "Testing Accuracy: 0.7829242945079543\n"
     ]
    }
   ],
   "source": [
    "#SVC_final = SVC(max_iter=80,kernel='linear',C=1)\n",
    "SVC_final = SVC(max_iter=80,kernel='linear',C=1)\n",
    "\n",
    "SVC_final.fit(X_trtf, y_train)\n",
    "#prediction = losso_final.predict(X_test)\n",
    "print(\"Training Accuracy: {}\".format(SVC_final.score(X_trtf, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(SVC_final.score(X_tstf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.584186308573575\n",
      "Test Recall: 0.21491000794555568\n",
      "F1 Score: 0.31422365895545007\n"
     ]
    }
   ],
   "source": [
    "predicted = SVC_final.predict(X_tstf)\n",
    "\n",
    "\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#alphas_alt = {'n_estimators': [2500,3000,3500,10000,50000,100000],\n",
    "#             'max_depth':[5,6],\n",
    "#             'max_features': ['log2',None],\n",
    "#              'min_samples_leaf':[3,5,7],\n",
    "#              'min_samples_split':[8,10,12]\n",
    "#          'min_weight_fraction_leaf':[]\n",
    "            #  'loss':['huber','quantile'],\n",
    "            #  'random_state':[42]\n",
    "#             }\n",
    "alphas_alt = {'n_estimators': [100,300,500],\n",
    "             'max_depth':[2,3],\n",
    "             'max_features': ['log2',None],\n",
    "              'min_samples_leaf':[3,5,7],\n",
    "              'min_samples_split':[8,10,12]\n",
    "#          'min_weight_fraction_leaf':[]\n",
    "            #  'loss':['huber','quantile'],\n",
    "            #  'random_state':[42]\n",
    "             }\n",
    "RandomForest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  2.6min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest classifier Model : {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'log2', 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "RandomForest = RandomizedSearchCV(estimator = RandomForest, param_distributions =alphas_alt, n_iter=3, cv = 3, verbose=2,n_jobs=-1, random_state=42)\n",
    "RandomForest.fit(X_trtf, y_train)\n",
    "\n",
    "print('Best parameters for RandomForest classifier Model : {}'.format(RandomForest.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7693906550722784\n",
      "Testing Accuracy: 0.7685906147573747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "RandomForest_final = RandomForestClassifier(n_estimators=500,max_depth=3,min_samples_split=10,min_samples_leaf=7,max_features='log2',n_jobs=-1)\n",
    "\n",
    "RandomForest_final.fit(X_trtf, y_train)\n",
    "print(\"Training Accuracy: {}\".format(RandomForest_final.score(X_trtf, y_train)))\n",
    "print(\"Testing Accuracy: {}\".format(RandomForest_final.score(X_tstf, y_test)))\n",
    "\n",
    "predicted = RandomForest_final.predict(X_tstf)\n",
    "print(\"Test Precision: {}\".format(precision_score(y_test, predicted)))\n",
    "print(\"Test Recall: {}\".format(recall_score(y_test, predicted)))\n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refrence:\n",
    "\n",
    "contraction_mapping:\n",
    "https://www.kaggle.com/nevermoi/jigsaw-toxic-prediction-by-simple-linearsvr-tfidf    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
