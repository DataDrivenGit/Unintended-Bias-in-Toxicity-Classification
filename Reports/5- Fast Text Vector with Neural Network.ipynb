{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is a complex field which is hypothesised to be part of AI-complete set of problems, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem of making computers as intelligent as people. With over 90% of data ever generated being produced in the last 2 years and with a great proportion being human generated unstructured text there is an ever increasing need to advance the field of Natural Language Processing.\n",
    "\n",
    "Recent UK Government proposal to have measures to regulate social media companies over harmful content, including \"substantial\" fines and the ability to block services that do not stick to the rules is an example of the regulamentary need to better manage the content that is being generated by users.\n",
    "\n",
    "Other initiatives like ​Riot Games​' work aimed to predict and reform toxic player behaviour during games is another example of this effort to understand the content being generated by users and moderate toxic content.\n",
    "However, as highlighted by the Kaggle competition ​Jigsaw unintended bias in toxicity classification​, existing models suffer from unintended bias where models might predict high likelihood of toxicity for content containing certain words (e.g. \"gay\") even when those comments were not actually toxic (such as \"I am a gay woman\"), leaving machine only classification models still sub-standard.\n",
    "\n",
    "The outcome of our analysis is the type of algorithm that companies will use to define what is free speech and what shouldn't be tolerated in a discussion. This challenge actually starts with how the training dataset was produced: Multiple people (annotators) read thousands of comments and defined if those comments were offensive or not. Where is the trick? They disagreed in many of them. Having tools that are able to flag up toxic content without suffering from unintended bias is of paramount importance to preserve Internet's fairness and freedom of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n",
    "\n",
    "In the data supplied for this competition, the text of the individual comment is found in the comment_text column. Each comment in Train has a toxicity label (target), and models should predict the target toxicity for the Test data. This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment.\n",
    "\n",
    "For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning:\n",
      "\n",
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "\n",
      "C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import spacy\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "\n",
    "import watermark\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "import operator\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])#\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler,robust_scale,MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer,sent_tokenize, word_tokenize \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, add\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "\n",
    "#print(os.path.abspath(PROJ_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('../Data/train.csv')\n",
    "#test= pd.read_csv('../Data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = np.where(train['target'] >= 0.5, True, False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Is_toxic'] =  train_df['target'].apply(lambda x: \"Toxic\" if x>=0.5 else \"NonToxic\")\n",
    "Nontoxic_df = train_df.loc[train_df['Is_toxic'] == 'NonToxic']\n",
    "Nontoxic_df = Nontoxic_df.head(481113)\n",
    "#Nontoxic_df = Nontoxic_df.head(30000)\n",
    "\n",
    "toxic_df = train_df.loc[train_df['Is_toxic'] == 'Toxic']\n",
    "#toxic_df = toxic_df.head(10000)\n",
    "#del final_df\n",
    "final_df = pd.concat([Nontoxic_df,toxic_df])\n",
    "#Nontoxic_df.append(toxic_df) \n",
    "#len(final_df)\n",
    "#dir()\n",
    "train=final_df\n",
    "del Nontoxic_df\n",
    "del toxic_df\n",
    "del train_df\n",
    "del final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp, test_temp = train_test_split(train, test_size=0.1, stratify=train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562902, 46)\n",
      "(62545, 46)\n"
     ]
    }
   ],
   "source": [
    "#identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "#for col in identity_columns + ['target']:\n",
    "print(train_temp.shape)\n",
    "print(test_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_temp['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train in train and validate\n",
    "train_df, valid_df = train_test_split(train_temp, test_size=0.33, stratify=train_temp['target'])\n",
    "test_df=test_temp\n",
    "#train_df=train_df[:250000]\n",
    "train_df=train_df\n",
    "train_df.loc[:,'set_']=\"train\"\n",
    "valid_df.loc[:,'set_']=\"valid\"\n",
    "test_df.loc[:,'set_']=\"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.33, stratify=train['target'])\n",
    "#test_df=null\n",
    "#train_df=train_df[:500000]\n",
    "#train_df=train_df\n",
    "\n",
    "train_df.loc[:,'set_']=\"train\"\n",
    "valid_df.loc[:,'set_']=\"valid\"\n",
    "#test_df.loc[:,'set_']=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set_indices=train_df.loc[:,'set_'][:500000]\n",
    "Set_indices=train_df.loc[:,'set_']\n",
    "Set_indices=Set_indices.append(valid_df.loc[:,'set_'])\n",
    "Set_indices=Set_indices.append(test_df.loc[:,'set_'])\n",
    "\n",
    "\n",
    "#y_train = train_df['target'][:500000]\n",
    "y_train = train_df['target']\n",
    "y_valid = valid_df['target']\n",
    "\n",
    "#Set_indices_labels=train_df.loc[:,'set_'][:500000]\n",
    "Set_indices_labels=train_df.loc[:,'set_']\n",
    "Set_indices_labels=Set_indices_labels.append(valid_df.loc[:,'set_'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625447,)\n",
      "(562902,)\n"
     ]
    }
   ],
   "source": [
    "texts=train_df['comment_text']\n",
    "texts=texts.append(valid_df['comment_text'])\n",
    "texts=texts.append(test_df['comment_text'])\n",
    "\n",
    "print(texts.shape)\n",
    "\n",
    "labels=train_df['target']\n",
    "labels=labels.append(valid_df['target'])\n",
    "\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(2)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>Is_toxic</th>\n",
       "      <th>set_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88413</th>\n",
       "      <td>350842</td>\n",
       "      <td>False</td>\n",
       "      <td>EXTRA EXTRA\\n\\nTHIS JUST IN\\n\\nThis downgrade ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NonToxic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512018</th>\n",
       "      <td>870502</td>\n",
       "      <td>False</td>\n",
       "      <td>She responded to Alaskans by being the main su...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NonToxic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413442</th>\n",
       "      <td>749166</td>\n",
       "      <td>False</td>\n",
       "      <td>With no relatives to leave anything to (always...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NonToxic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225638</th>\n",
       "      <td>518863</td>\n",
       "      <td>False</td>\n",
       "      <td>She did just fine.  She doesn't need to \"do be...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NonToxic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470084</th>\n",
       "      <td>820659</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh my gosh. So, so sad.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NonToxic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  target                                       comment_text  \\\n",
       "88413   350842   False  EXTRA EXTRA\\n\\nTHIS JUST IN\\n\\nThis downgrade ...   \n",
       "512018  870502   False  She responded to Alaskans by being the main su...   \n",
       "413442  749166   False  With no relatives to leave anything to (always...   \n",
       "225638  518863   False  She did just fine.  She doesn't need to \"do be...   \n",
       "470084  820659   False                            Oh my gosh. So, so sad.   \n",
       "\n",
       "        severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "88413               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "512018              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "413442              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "225638              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "470084              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "        atheist  ...  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "88413       NaN  ...      0    0    0      2         0              0.0   \n",
       "512018      NaN  ...      0    1    0      1         0              0.0   \n",
       "413442      NaN  ...      0    0    0      3         3              0.0   \n",
       "225638      NaN  ...      0    0    0      3         0              0.0   \n",
       "470084      NaN  ...      0    0    0      0         0              0.0   \n",
       "\n",
       "        identity_annotator_count  toxicity_annotator_count  Is_toxic   set_  \n",
       "88413                          0                         4  NonToxic  train  \n",
       "512018                         0                         4  NonToxic  train  \n",
       "413442                         0                         4  NonToxic  train  \n",
       "225638                         0                         4  NonToxic  train  \n",
       "470084                         0                         4  NonToxic  train  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 150\n",
    "max_words = 90000\n",
    "embedding_size=300\n",
    "lr = 1e-3\n",
    "lr_d = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216677 unique tokens.\n",
      "Shape of data tensor: (625447, 150)\n",
      "Shape of label tensor: (562902,)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train tensor: (377144, 150)\n",
      "Shape of validate tensor: (185758, 150)\n",
      "Shape of test tensor: (62545, 150)\n"
     ]
    }
   ],
   "source": [
    "x_train = data[Set_indices == \"train\"]\n",
    "x_val = data[Set_indices == \"valid\"]\n",
    "x_test = data[Set_indices == \"test\"]\n",
    "\n",
    "\n",
    "y_train = labels[Set_indices_labels == \"train\"]\n",
    "y_val = labels[Set_indices_labels == \"valid\"]\n",
    "\n",
    "print('Shape of train tensor:', x_train.shape)\n",
    "print('Shape of validate tensor:', x_val.shape)\n",
    "print('Shape of test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = embedding_size\n",
    "max_features = max_words\n",
    "\n",
    "embedding_path = \"../crawl300d/crawl-300d-2M.vec\"\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path,encoding=\"utf8\"))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90001, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 300)          27000300  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 45000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1440032   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 28,440,365\n",
      "Trainable params: 28,440,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words+1, embed_size, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable = False\n",
    "\n",
    "#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parth\\Anaconda3\\envs\\python3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 377144 samples, validate on 185758 samples\n",
      "Epoch 1/10\n",
      "377144/377144 [==============================] - ETA: 2:14 - loss: 0.7347 - acc: 0.307 - ETA: 1:12 - loss: 0.6696 - acc: 0.537 - ETA: 52s - loss: 0.6510 - acc: 0.612 - ETA: 41s - loss: 0.6297 - acc: 0.65 - ETA: 35s - loss: 0.6099 - acc: 0.67 - ETA: 30s - loss: 0.5938 - acc: 0.69 - ETA: 27s - loss: 0.5817 - acc: 0.70 - ETA: 25s - loss: 0.5723 - acc: 0.71 - ETA: 23s - loss: 0.5628 - acc: 0.71 - ETA: 21s - loss: 0.5545 - acc: 0.72 - ETA: 20s - loss: 0.5482 - acc: 0.72 - ETA: 19s - loss: 0.5418 - acc: 0.73 - ETA: 18s - loss: 0.5368 - acc: 0.73 - ETA: 17s - loss: 0.5298 - acc: 0.73 - ETA: 17s - loss: 0.5248 - acc: 0.73 - ETA: 16s - loss: 0.5199 - acc: 0.73 - ETA: 15s - loss: 0.5151 - acc: 0.74 - ETA: 15s - loss: 0.5101 - acc: 0.74 - ETA: 14s - loss: 0.5059 - acc: 0.74 - ETA: 14s - loss: 0.5024 - acc: 0.74 - ETA: 13s - loss: 0.4995 - acc: 0.74 - ETA: 13s - loss: 0.4958 - acc: 0.74 - ETA: 12s - loss: 0.4923 - acc: 0.74 - ETA: 12s - loss: 0.4892 - acc: 0.74 - ETA: 11s - loss: 0.4858 - acc: 0.75 - ETA: 11s - loss: 0.4828 - acc: 0.75 - ETA: 11s - loss: 0.4798 - acc: 0.75 - ETA: 10s - loss: 0.4765 - acc: 0.75 - ETA: 10s - loss: 0.4739 - acc: 0.75 - ETA: 10s - loss: 0.4721 - acc: 0.75 - ETA: 9s - loss: 0.4697 - acc: 0.7540 - ETA: 9s - loss: 0.4677 - acc: 0.754 - ETA: 9s - loss: 0.4652 - acc: 0.754 - ETA: 9s - loss: 0.4631 - acc: 0.755 - ETA: 8s - loss: 0.4608 - acc: 0.756 - ETA: 8s - loss: 0.4587 - acc: 0.756 - ETA: 8s - loss: 0.4564 - acc: 0.756 - ETA: 8s - loss: 0.4544 - acc: 0.757 - ETA: 7s - loss: 0.4522 - acc: 0.757 - ETA: 7s - loss: 0.4505 - acc: 0.757 - ETA: 7s - loss: 0.4489 - acc: 0.757 - ETA: 7s - loss: 0.4471 - acc: 0.759 - ETA: 6s - loss: 0.4455 - acc: 0.761 - ETA: 6s - loss: 0.4437 - acc: 0.763 - ETA: 6s - loss: 0.4420 - acc: 0.764 - ETA: 6s - loss: 0.4408 - acc: 0.766 - ETA: 5s - loss: 0.4393 - acc: 0.767 - ETA: 5s - loss: 0.4379 - acc: 0.768 - ETA: 5s - loss: 0.4365 - acc: 0.770 - ETA: 5s - loss: 0.4349 - acc: 0.771 - ETA: 5s - loss: 0.4335 - acc: 0.772 - ETA: 4s - loss: 0.4321 - acc: 0.774 - ETA: 4s - loss: 0.4309 - acc: 0.775 - ETA: 4s - loss: 0.4298 - acc: 0.776 - ETA: 4s - loss: 0.4284 - acc: 0.777 - ETA: 3s - loss: 0.4269 - acc: 0.779 - ETA: 3s - loss: 0.4258 - acc: 0.780 - ETA: 3s - loss: 0.4246 - acc: 0.781 - ETA: 3s - loss: 0.4235 - acc: 0.782 - ETA: 3s - loss: 0.4223 - acc: 0.783 - ETA: 2s - loss: 0.4212 - acc: 0.784 - ETA: 2s - loss: 0.4201 - acc: 0.785 - ETA: 2s - loss: 0.4189 - acc: 0.787 - ETA: 2s - loss: 0.4178 - acc: 0.788 - ETA: 2s - loss: 0.4168 - acc: 0.789 - ETA: 1s - loss: 0.4160 - acc: 0.789 - ETA: 1s - loss: 0.4149 - acc: 0.790 - ETA: 1s - loss: 0.4140 - acc: 0.791 - ETA: 1s - loss: 0.4130 - acc: 0.792 - ETA: 1s - loss: 0.4121 - acc: 0.793 - ETA: 0s - loss: 0.4112 - acc: 0.794 - ETA: 0s - loss: 0.4102 - acc: 0.795 - ETA: 0s - loss: 0.4093 - acc: 0.796 - ETA: 0s - loss: 0.4085 - acc: 0.797 - ETA: 0s - loss: 0.4076 - acc: 0.797 - 17s 46us/step - loss: 0.4073 - acc: 0.7982 - val_loss: 0.3402 - val_acc: 0.8645\n",
      "Epoch 2/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.3089 - acc: 0.89 - ETA: 14s - loss: 0.3143 - acc: 0.88 - ETA: 14s - loss: 0.3140 - acc: 0.87 - ETA: 14s - loss: 0.3173 - acc: 0.87 - ETA: 13s - loss: 0.3156 - acc: 0.87 - ETA: 13s - loss: 0.3144 - acc: 0.87 - ETA: 13s - loss: 0.3144 - acc: 0.87 - ETA: 13s - loss: 0.3144 - acc: 0.87 - ETA: 13s - loss: 0.3135 - acc: 0.87 - ETA: 13s - loss: 0.3135 - acc: 0.87 - ETA: 13s - loss: 0.3142 - acc: 0.87 - ETA: 12s - loss: 0.3141 - acc: 0.87 - ETA: 12s - loss: 0.3147 - acc: 0.87 - ETA: 12s - loss: 0.3145 - acc: 0.87 - ETA: 12s - loss: 0.3141 - acc: 0.87 - ETA: 12s - loss: 0.3136 - acc: 0.87 - ETA: 11s - loss: 0.3133 - acc: 0.87 - ETA: 11s - loss: 0.3129 - acc: 0.87 - ETA: 11s - loss: 0.3124 - acc: 0.87 - ETA: 11s - loss: 0.3113 - acc: 0.87 - ETA: 11s - loss: 0.3109 - acc: 0.87 - ETA: 10s - loss: 0.3107 - acc: 0.87 - ETA: 10s - loss: 0.3106 - acc: 0.87 - ETA: 10s - loss: 0.3103 - acc: 0.87 - ETA: 10s - loss: 0.3100 - acc: 0.87 - ETA: 10s - loss: 0.3100 - acc: 0.87 - ETA: 9s - loss: 0.3097 - acc: 0.8774 - ETA: 9s - loss: 0.3095 - acc: 0.877 - ETA: 9s - loss: 0.3091 - acc: 0.878 - ETA: 9s - loss: 0.3086 - acc: 0.878 - ETA: 9s - loss: 0.3087 - acc: 0.878 - ETA: 8s - loss: 0.3086 - acc: 0.878 - ETA: 8s - loss: 0.3082 - acc: 0.879 - ETA: 8s - loss: 0.3074 - acc: 0.879 - ETA: 8s - loss: 0.3070 - acc: 0.880 - ETA: 8s - loss: 0.3070 - acc: 0.880 - ETA: 7s - loss: 0.3068 - acc: 0.880 - ETA: 7s - loss: 0.3065 - acc: 0.880 - ETA: 7s - loss: 0.3064 - acc: 0.881 - ETA: 7s - loss: 0.3061 - acc: 0.881 - ETA: 7s - loss: 0.3059 - acc: 0.881 - ETA: 6s - loss: 0.3058 - acc: 0.882 - ETA: 6s - loss: 0.3056 - acc: 0.882 - ETA: 6s - loss: 0.3052 - acc: 0.882 - ETA: 6s - loss: 0.3045 - acc: 0.882 - ETA: 5s - loss: 0.3044 - acc: 0.882 - ETA: 5s - loss: 0.3041 - acc: 0.883 - ETA: 5s - loss: 0.3039 - acc: 0.883 - ETA: 5s - loss: 0.3039 - acc: 0.883 - ETA: 5s - loss: 0.3037 - acc: 0.883 - ETA: 4s - loss: 0.3035 - acc: 0.883 - ETA: 4s - loss: 0.3035 - acc: 0.884 - ETA: 4s - loss: 0.3029 - acc: 0.884 - ETA: 4s - loss: 0.3028 - acc: 0.884 - ETA: 4s - loss: 0.3025 - acc: 0.884 - ETA: 3s - loss: 0.3023 - acc: 0.884 - ETA: 3s - loss: 0.3021 - acc: 0.884 - ETA: 3s - loss: 0.3020 - acc: 0.885 - ETA: 3s - loss: 0.3018 - acc: 0.885 - ETA: 3s - loss: 0.3014 - acc: 0.885 - ETA: 2s - loss: 0.3014 - acc: 0.885 - ETA: 2s - loss: 0.3011 - acc: 0.885 - ETA: 2s - loss: 0.3009 - acc: 0.886 - ETA: 2s - loss: 0.3006 - acc: 0.886 - ETA: 2s - loss: 0.3003 - acc: 0.886 - ETA: 1s - loss: 0.3002 - acc: 0.886 - ETA: 1s - loss: 0.3001 - acc: 0.886 - ETA: 1s - loss: 0.2999 - acc: 0.886 - ETA: 1s - loss: 0.2998 - acc: 0.887 - ETA: 1s - loss: 0.2997 - acc: 0.887 - ETA: 0s - loss: 0.2996 - acc: 0.887 - ETA: 0s - loss: 0.2994 - acc: 0.887 - ETA: 0s - loss: 0.2993 - acc: 0.887 - ETA: 0s - loss: 0.2991 - acc: 0.887 - ETA: 0s - loss: 0.2989 - acc: 0.887 - 17s 46us/step - loss: 0.2989 - acc: 0.8878 - val_loss: 0.3071 - val_acc: 0.8873\n",
      "Epoch 3/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.2532 - acc: 0.91 - ETA: 15s - loss: 0.2514 - acc: 0.91 - ETA: 15s - loss: 0.2525 - acc: 0.91 - ETA: 14s - loss: 0.2540 - acc: 0.91 - ETA: 14s - loss: 0.2556 - acc: 0.91 - ETA: 14s - loss: 0.2569 - acc: 0.91 - ETA: 14s - loss: 0.2569 - acc: 0.91 - ETA: 14s - loss: 0.2560 - acc: 0.91 - ETA: 14s - loss: 0.2560 - acc: 0.91 - ETA: 13s - loss: 0.2555 - acc: 0.91 - ETA: 13s - loss: 0.2548 - acc: 0.91 - ETA: 13s - loss: 0.2542 - acc: 0.91 - ETA: 12s - loss: 0.2538 - acc: 0.91 - ETA: 12s - loss: 0.2541 - acc: 0.91 - ETA: 12s - loss: 0.2540 - acc: 0.91 - ETA: 12s - loss: 0.2540 - acc: 0.91 - ETA: 12s - loss: 0.2540 - acc: 0.91 - ETA: 11s - loss: 0.2538 - acc: 0.91 - ETA: 11s - loss: 0.2532 - acc: 0.91 - ETA: 11s - loss: 0.2531 - acc: 0.91 - ETA: 11s - loss: 0.2528 - acc: 0.91 - ETA: 10s - loss: 0.2527 - acc: 0.91 - ETA: 10s - loss: 0.2528 - acc: 0.91 - ETA: 10s - loss: 0.2530 - acc: 0.91 - ETA: 10s - loss: 0.2525 - acc: 0.91 - ETA: 10s - loss: 0.2526 - acc: 0.91 - ETA: 9s - loss: 0.2524 - acc: 0.9146 - ETA: 9s - loss: 0.2524 - acc: 0.914 - ETA: 9s - loss: 0.2524 - acc: 0.915 - ETA: 9s - loss: 0.2521 - acc: 0.915 - ETA: 9s - loss: 0.2524 - acc: 0.914 - ETA: 8s - loss: 0.2522 - acc: 0.915 - ETA: 8s - loss: 0.2524 - acc: 0.915 - ETA: 8s - loss: 0.2526 - acc: 0.915 - ETA: 8s - loss: 0.2526 - acc: 0.915 - ETA: 8s - loss: 0.2525 - acc: 0.915 - ETA: 7s - loss: 0.2527 - acc: 0.915 - ETA: 7s - loss: 0.2530 - acc: 0.915 - ETA: 7s - loss: 0.2528 - acc: 0.915 - ETA: 7s - loss: 0.2528 - acc: 0.915 - ETA: 7s - loss: 0.2530 - acc: 0.915 - ETA: 6s - loss: 0.2529 - acc: 0.915 - ETA: 6s - loss: 0.2527 - acc: 0.915 - ETA: 6s - loss: 0.2526 - acc: 0.915 - ETA: 6s - loss: 0.2525 - acc: 0.915 - ETA: 6s - loss: 0.2526 - acc: 0.915 - ETA: 5s - loss: 0.2529 - acc: 0.915 - ETA: 5s - loss: 0.2529 - acc: 0.915 - ETA: 5s - loss: 0.2527 - acc: 0.915 - ETA: 5s - loss: 0.2526 - acc: 0.915 - ETA: 5s - loss: 0.2524 - acc: 0.915 - ETA: 4s - loss: 0.2523 - acc: 0.915 - ETA: 4s - loss: 0.2522 - acc: 0.915 - ETA: 4s - loss: 0.2520 - acc: 0.915 - ETA: 4s - loss: 0.2519 - acc: 0.915 - ETA: 3s - loss: 0.2519 - acc: 0.915 - ETA: 3s - loss: 0.2521 - acc: 0.915 - ETA: 3s - loss: 0.2520 - acc: 0.915 - ETA: 3s - loss: 0.2519 - acc: 0.916 - ETA: 3s - loss: 0.2517 - acc: 0.916 - ETA: 2s - loss: 0.2517 - acc: 0.916 - ETA: 2s - loss: 0.2517 - acc: 0.916 - ETA: 2s - loss: 0.2515 - acc: 0.916 - ETA: 2s - loss: 0.2515 - acc: 0.916 - ETA: 2s - loss: 0.2514 - acc: 0.916 - ETA: 1s - loss: 0.2516 - acc: 0.916 - ETA: 1s - loss: 0.2516 - acc: 0.916 - ETA: 1s - loss: 0.2517 - acc: 0.916 - ETA: 1s - loss: 0.2517 - acc: 0.916 - ETA: 1s - loss: 0.2517 - acc: 0.916 - ETA: 0s - loss: 0.2516 - acc: 0.916 - ETA: 0s - loss: 0.2516 - acc: 0.916 - ETA: 0s - loss: 0.2516 - acc: 0.916 - ETA: 0s - loss: 0.2515 - acc: 0.916 - ETA: 0s - loss: 0.2514 - acc: 0.916 - 18s 46us/step - loss: 0.2513 - acc: 0.9164 - val_loss: 0.3048 - val_acc: 0.8923\n",
      "Epoch 4/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.2229 - acc: 0.93 - ETA: 15s - loss: 0.2218 - acc: 0.93 - ETA: 14s - loss: 0.2184 - acc: 0.93 - ETA: 14s - loss: 0.2170 - acc: 0.93 - ETA: 14s - loss: 0.2170 - acc: 0.93 - ETA: 14s - loss: 0.2164 - acc: 0.93 - ETA: 13s - loss: 0.2183 - acc: 0.93 - ETA: 13s - loss: 0.2187 - acc: 0.93 - ETA: 13s - loss: 0.2178 - acc: 0.93 - ETA: 13s - loss: 0.2186 - acc: 0.93 - ETA: 13s - loss: 0.2190 - acc: 0.93 - ETA: 13s - loss: 0.2190 - acc: 0.93 - ETA: 12s - loss: 0.2192 - acc: 0.93 - ETA: 12s - loss: 0.2197 - acc: 0.93 - ETA: 12s - loss: 0.2196 - acc: 0.93 - ETA: 12s - loss: 0.2193 - acc: 0.93 - ETA: 12s - loss: 0.2188 - acc: 0.93 - ETA: 11s - loss: 0.2183 - acc: 0.93 - ETA: 11s - loss: 0.2189 - acc: 0.93 - ETA: 11s - loss: 0.2183 - acc: 0.93 - ETA: 11s - loss: 0.2180 - acc: 0.93 - ETA: 10s - loss: 0.2180 - acc: 0.93 - ETA: 10s - loss: 0.2182 - acc: 0.93 - ETA: 10s - loss: 0.2182 - acc: 0.93 - ETA: 10s - loss: 0.2179 - acc: 0.93 - ETA: 10s - loss: 0.2177 - acc: 0.93 - ETA: 10s - loss: 0.2180 - acc: 0.93 - ETA: 9s - loss: 0.2179 - acc: 0.9342 - ETA: 9s - loss: 0.2179 - acc: 0.934 - ETA: 9s - loss: 0.2180 - acc: 0.934 - ETA: 9s - loss: 0.2179 - acc: 0.934 - ETA: 8s - loss: 0.2176 - acc: 0.934 - ETA: 8s - loss: 0.2175 - acc: 0.934 - ETA: 8s - loss: 0.2176 - acc: 0.934 - ETA: 8s - loss: 0.2177 - acc: 0.934 - ETA: 8s - loss: 0.2177 - acc: 0.934 - ETA: 7s - loss: 0.2175 - acc: 0.934 - ETA: 7s - loss: 0.2173 - acc: 0.934 - ETA: 7s - loss: 0.2172 - acc: 0.934 - ETA: 7s - loss: 0.2173 - acc: 0.934 - ETA: 7s - loss: 0.2174 - acc: 0.934 - ETA: 6s - loss: 0.2176 - acc: 0.934 - ETA: 6s - loss: 0.2174 - acc: 0.934 - ETA: 6s - loss: 0.2175 - acc: 0.934 - ETA: 6s - loss: 0.2174 - acc: 0.934 - ETA: 6s - loss: 0.2174 - acc: 0.934 - ETA: 5s - loss: 0.2175 - acc: 0.934 - ETA: 5s - loss: 0.2174 - acc: 0.934 - ETA: 5s - loss: 0.2175 - acc: 0.934 - ETA: 5s - loss: 0.2174 - acc: 0.934 - ETA: 5s - loss: 0.2175 - acc: 0.934 - ETA: 4s - loss: 0.2175 - acc: 0.934 - ETA: 4s - loss: 0.2175 - acc: 0.934 - ETA: 4s - loss: 0.2175 - acc: 0.934 - ETA: 4s - loss: 0.2174 - acc: 0.935 - ETA: 4s - loss: 0.2175 - acc: 0.934 - ETA: 3s - loss: 0.2174 - acc: 0.934 - ETA: 3s - loss: 0.2173 - acc: 0.934 - ETA: 3s - loss: 0.2174 - acc: 0.934 - ETA: 3s - loss: 0.2174 - acc: 0.934 - ETA: 2s - loss: 0.2172 - acc: 0.934 - ETA: 2s - loss: 0.2173 - acc: 0.934 - ETA: 2s - loss: 0.2174 - acc: 0.934 - ETA: 2s - loss: 0.2174 - acc: 0.934 - ETA: 2s - loss: 0.2175 - acc: 0.934 - ETA: 1s - loss: 0.2175 - acc: 0.934 - ETA: 1s - loss: 0.2175 - acc: 0.934 - ETA: 1s - loss: 0.2175 - acc: 0.934 - ETA: 1s - loss: 0.2175 - acc: 0.934 - ETA: 1s - loss: 0.2175 - acc: 0.934 - ETA: 0s - loss: 0.2175 - acc: 0.934 - ETA: 0s - loss: 0.2174 - acc: 0.934 - ETA: 0s - loss: 0.2176 - acc: 0.934 - ETA: 0s - loss: 0.2175 - acc: 0.934 - ETA: 0s - loss: 0.2175 - acc: 0.934 - 18s 47us/step - loss: 0.2175 - acc: 0.9347 - val_loss: 0.3120 - val_acc: 0.8948\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377144/377144 [==============================] - ETA: 14s - loss: 0.1972 - acc: 0.95 - ETA: 14s - loss: 0.1920 - acc: 0.94 - ETA: 14s - loss: 0.1963 - acc: 0.94 - ETA: 14s - loss: 0.1928 - acc: 0.94 - ETA: 14s - loss: 0.1923 - acc: 0.95 - ETA: 13s - loss: 0.1925 - acc: 0.95 - ETA: 13s - loss: 0.1909 - acc: 0.95 - ETA: 13s - loss: 0.1920 - acc: 0.94 - ETA: 13s - loss: 0.1924 - acc: 0.94 - ETA: 13s - loss: 0.1922 - acc: 0.94 - ETA: 13s - loss: 0.1920 - acc: 0.95 - ETA: 12s - loss: 0.1915 - acc: 0.95 - ETA: 12s - loss: 0.1920 - acc: 0.94 - ETA: 12s - loss: 0.1920 - acc: 0.94 - ETA: 12s - loss: 0.1918 - acc: 0.94 - ETA: 12s - loss: 0.1918 - acc: 0.94 - ETA: 11s - loss: 0.1916 - acc: 0.94 - ETA: 11s - loss: 0.1909 - acc: 0.95 - ETA: 11s - loss: 0.1908 - acc: 0.95 - ETA: 11s - loss: 0.1909 - acc: 0.94 - ETA: 11s - loss: 0.1911 - acc: 0.95 - ETA: 10s - loss: 0.1914 - acc: 0.94 - ETA: 10s - loss: 0.1914 - acc: 0.94 - ETA: 10s - loss: 0.1913 - acc: 0.94 - ETA: 10s - loss: 0.1913 - acc: 0.94 - ETA: 10s - loss: 0.1914 - acc: 0.94 - ETA: 9s - loss: 0.1916 - acc: 0.9498 - ETA: 9s - loss: 0.1915 - acc: 0.950 - ETA: 9s - loss: 0.1913 - acc: 0.950 - ETA: 9s - loss: 0.1915 - acc: 0.949 - ETA: 9s - loss: 0.1913 - acc: 0.949 - ETA: 8s - loss: 0.1912 - acc: 0.949 - ETA: 8s - loss: 0.1910 - acc: 0.950 - ETA: 8s - loss: 0.1909 - acc: 0.950 - ETA: 8s - loss: 0.1908 - acc: 0.950 - ETA: 8s - loss: 0.1910 - acc: 0.949 - ETA: 7s - loss: 0.1913 - acc: 0.949 - ETA: 7s - loss: 0.1914 - acc: 0.949 - ETA: 7s - loss: 0.1912 - acc: 0.950 - ETA: 7s - loss: 0.1914 - acc: 0.949 - ETA: 7s - loss: 0.1917 - acc: 0.949 - ETA: 6s - loss: 0.1916 - acc: 0.949 - ETA: 6s - loss: 0.1916 - acc: 0.949 - ETA: 6s - loss: 0.1915 - acc: 0.949 - ETA: 6s - loss: 0.1915 - acc: 0.949 - ETA: 5s - loss: 0.1915 - acc: 0.949 - ETA: 5s - loss: 0.1915 - acc: 0.949 - ETA: 5s - loss: 0.1914 - acc: 0.949 - ETA: 5s - loss: 0.1913 - acc: 0.949 - ETA: 5s - loss: 0.1914 - acc: 0.949 - ETA: 4s - loss: 0.1913 - acc: 0.949 - ETA: 4s - loss: 0.1912 - acc: 0.949 - ETA: 4s - loss: 0.1912 - acc: 0.949 - ETA: 4s - loss: 0.1912 - acc: 0.949 - ETA: 4s - loss: 0.1914 - acc: 0.949 - ETA: 3s - loss: 0.1914 - acc: 0.949 - ETA: 3s - loss: 0.1914 - acc: 0.949 - ETA: 3s - loss: 0.1915 - acc: 0.949 - ETA: 3s - loss: 0.1915 - acc: 0.949 - ETA: 3s - loss: 0.1915 - acc: 0.949 - ETA: 2s - loss: 0.1915 - acc: 0.949 - ETA: 2s - loss: 0.1915 - acc: 0.949 - ETA: 2s - loss: 0.1915 - acc: 0.949 - ETA: 2s - loss: 0.1914 - acc: 0.949 - ETA: 2s - loss: 0.1914 - acc: 0.949 - ETA: 1s - loss: 0.1914 - acc: 0.949 - ETA: 1s - loss: 0.1914 - acc: 0.949 - ETA: 1s - loss: 0.1913 - acc: 0.949 - ETA: 1s - loss: 0.1912 - acc: 0.949 - ETA: 1s - loss: 0.1912 - acc: 0.949 - ETA: 0s - loss: 0.1912 - acc: 0.949 - ETA: 0s - loss: 0.1912 - acc: 0.949 - ETA: 0s - loss: 0.1912 - acc: 0.949 - ETA: 0s - loss: 0.1913 - acc: 0.949 - ETA: 0s - loss: 0.1913 - acc: 0.949 - 17s 46us/step - loss: 0.1912 - acc: 0.9490 - val_loss: 0.3309 - val_acc: 0.8935\n",
      "Epoch 6/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.1747 - acc: 0.96 - ETA: 14s - loss: 0.1719 - acc: 0.96 - ETA: 15s - loss: 0.1738 - acc: 0.95 - ETA: 15s - loss: 0.1738 - acc: 0.95 - ETA: 14s - loss: 0.1728 - acc: 0.95 - ETA: 14s - loss: 0.1725 - acc: 0.96 - ETA: 14s - loss: 0.1726 - acc: 0.96 - ETA: 13s - loss: 0.1717 - acc: 0.96 - ETA: 13s - loss: 0.1731 - acc: 0.95 - ETA: 13s - loss: 0.1733 - acc: 0.95 - ETA: 13s - loss: 0.1731 - acc: 0.95 - ETA: 13s - loss: 0.1729 - acc: 0.95 - ETA: 12s - loss: 0.1722 - acc: 0.96 - ETA: 12s - loss: 0.1722 - acc: 0.95 - ETA: 12s - loss: 0.1720 - acc: 0.95 - ETA: 12s - loss: 0.1717 - acc: 0.95 - ETA: 11s - loss: 0.1720 - acc: 0.96 - ETA: 11s - loss: 0.1722 - acc: 0.96 - ETA: 11s - loss: 0.1723 - acc: 0.96 - ETA: 11s - loss: 0.1720 - acc: 0.95 - ETA: 11s - loss: 0.1718 - acc: 0.95 - ETA: 10s - loss: 0.1718 - acc: 0.96 - ETA: 10s - loss: 0.1718 - acc: 0.96 - ETA: 10s - loss: 0.1717 - acc: 0.96 - ETA: 10s - loss: 0.1718 - acc: 0.96 - ETA: 10s - loss: 0.1718 - acc: 0.96 - ETA: 9s - loss: 0.1717 - acc: 0.9605 - ETA: 9s - loss: 0.1714 - acc: 0.960 - ETA: 9s - loss: 0.1713 - acc: 0.960 - ETA: 9s - loss: 0.1714 - acc: 0.960 - ETA: 9s - loss: 0.1715 - acc: 0.960 - ETA: 8s - loss: 0.1714 - acc: 0.960 - ETA: 8s - loss: 0.1713 - acc: 0.960 - ETA: 8s - loss: 0.1713 - acc: 0.960 - ETA: 8s - loss: 0.1714 - acc: 0.960 - ETA: 8s - loss: 0.1716 - acc: 0.960 - ETA: 7s - loss: 0.1717 - acc: 0.960 - ETA: 7s - loss: 0.1715 - acc: 0.960 - ETA: 7s - loss: 0.1714 - acc: 0.960 - ETA: 7s - loss: 0.1712 - acc: 0.960 - ETA: 7s - loss: 0.1710 - acc: 0.960 - ETA: 6s - loss: 0.1709 - acc: 0.960 - ETA: 6s - loss: 0.1710 - acc: 0.960 - ETA: 6s - loss: 0.1708 - acc: 0.960 - ETA: 6s - loss: 0.1708 - acc: 0.960 - ETA: 5s - loss: 0.1709 - acc: 0.960 - ETA: 5s - loss: 0.1707 - acc: 0.960 - ETA: 5s - loss: 0.1706 - acc: 0.960 - ETA: 5s - loss: 0.1707 - acc: 0.960 - ETA: 5s - loss: 0.1708 - acc: 0.960 - ETA: 4s - loss: 0.1708 - acc: 0.960 - ETA: 4s - loss: 0.1708 - acc: 0.960 - ETA: 4s - loss: 0.1708 - acc: 0.960 - ETA: 4s - loss: 0.1708 - acc: 0.960 - ETA: 4s - loss: 0.1708 - acc: 0.960 - ETA: 3s - loss: 0.1706 - acc: 0.960 - ETA: 3s - loss: 0.1706 - acc: 0.960 - ETA: 3s - loss: 0.1706 - acc: 0.960 - ETA: 3s - loss: 0.1705 - acc: 0.960 - ETA: 3s - loss: 0.1706 - acc: 0.960 - ETA: 2s - loss: 0.1705 - acc: 0.960 - ETA: 2s - loss: 0.1705 - acc: 0.960 - ETA: 2s - loss: 0.1704 - acc: 0.960 - ETA: 2s - loss: 0.1704 - acc: 0.960 - ETA: 2s - loss: 0.1704 - acc: 0.960 - ETA: 1s - loss: 0.1704 - acc: 0.960 - ETA: 1s - loss: 0.1703 - acc: 0.960 - ETA: 1s - loss: 0.1702 - acc: 0.960 - ETA: 1s - loss: 0.1703 - acc: 0.960 - ETA: 1s - loss: 0.1703 - acc: 0.960 - ETA: 0s - loss: 0.1703 - acc: 0.960 - ETA: 0s - loss: 0.1702 - acc: 0.960 - ETA: 0s - loss: 0.1702 - acc: 0.960 - ETA: 0s - loss: 0.1702 - acc: 0.960 - ETA: 0s - loss: 0.1702 - acc: 0.960 - 17s 46us/step - loss: 0.1701 - acc: 0.9604 - val_loss: 0.3582 - val_acc: 0.8934\n",
      "Epoch 7/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.1540 - acc: 0.96 - ETA: 14s - loss: 0.1573 - acc: 0.96 - ETA: 14s - loss: 0.1553 - acc: 0.96 - ETA: 14s - loss: 0.1548 - acc: 0.96 - ETA: 14s - loss: 0.1543 - acc: 0.96 - ETA: 13s - loss: 0.1534 - acc: 0.97 - ETA: 13s - loss: 0.1538 - acc: 0.96 - ETA: 13s - loss: 0.1535 - acc: 0.96 - ETA: 13s - loss: 0.1533 - acc: 0.96 - ETA: 13s - loss: 0.1536 - acc: 0.96 - ETA: 13s - loss: 0.1540 - acc: 0.96 - ETA: 13s - loss: 0.1544 - acc: 0.96 - ETA: 12s - loss: 0.1543 - acc: 0.96 - ETA: 12s - loss: 0.1535 - acc: 0.96 - ETA: 12s - loss: 0.1534 - acc: 0.96 - ETA: 12s - loss: 0.1535 - acc: 0.96 - ETA: 11s - loss: 0.1539 - acc: 0.96 - ETA: 11s - loss: 0.1541 - acc: 0.96 - ETA: 11s - loss: 0.1545 - acc: 0.96 - ETA: 11s - loss: 0.1544 - acc: 0.96 - ETA: 11s - loss: 0.1543 - acc: 0.96 - ETA: 10s - loss: 0.1541 - acc: 0.96 - ETA: 10s - loss: 0.1540 - acc: 0.96 - ETA: 10s - loss: 0.1537 - acc: 0.96 - ETA: 10s - loss: 0.1538 - acc: 0.96 - ETA: 10s - loss: 0.1537 - acc: 0.96 - ETA: 9s - loss: 0.1534 - acc: 0.9698 - ETA: 9s - loss: 0.1532 - acc: 0.969 - ETA: 9s - loss: 0.1533 - acc: 0.969 - ETA: 9s - loss: 0.1533 - acc: 0.969 - ETA: 8s - loss: 0.1535 - acc: 0.969 - ETA: 8s - loss: 0.1533 - acc: 0.969 - ETA: 8s - loss: 0.1533 - acc: 0.969 - ETA: 8s - loss: 0.1532 - acc: 0.969 - ETA: 8s - loss: 0.1532 - acc: 0.969 - ETA: 8s - loss: 0.1530 - acc: 0.969 - ETA: 7s - loss: 0.1528 - acc: 0.969 - ETA: 7s - loss: 0.1530 - acc: 0.969 - ETA: 7s - loss: 0.1530 - acc: 0.969 - ETA: 7s - loss: 0.1530 - acc: 0.969 - ETA: 7s - loss: 0.1529 - acc: 0.969 - ETA: 6s - loss: 0.1528 - acc: 0.969 - ETA: 6s - loss: 0.1528 - acc: 0.969 - ETA: 6s - loss: 0.1529 - acc: 0.969 - ETA: 6s - loss: 0.1528 - acc: 0.969 - ETA: 6s - loss: 0.1528 - acc: 0.969 - ETA: 5s - loss: 0.1527 - acc: 0.969 - ETA: 5s - loss: 0.1528 - acc: 0.969 - ETA: 5s - loss: 0.1528 - acc: 0.969 - ETA: 5s - loss: 0.1528 - acc: 0.969 - ETA: 4s - loss: 0.1528 - acc: 0.969 - ETA: 4s - loss: 0.1528 - acc: 0.969 - ETA: 4s - loss: 0.1528 - acc: 0.969 - ETA: 4s - loss: 0.1528 - acc: 0.969 - ETA: 4s - loss: 0.1527 - acc: 0.969 - ETA: 3s - loss: 0.1528 - acc: 0.969 - ETA: 3s - loss: 0.1528 - acc: 0.969 - ETA: 3s - loss: 0.1528 - acc: 0.969 - ETA: 3s - loss: 0.1528 - acc: 0.969 - ETA: 3s - loss: 0.1529 - acc: 0.968 - ETA: 2s - loss: 0.1529 - acc: 0.969 - ETA: 2s - loss: 0.1529 - acc: 0.969 - ETA: 2s - loss: 0.1529 - acc: 0.968 - ETA: 2s - loss: 0.1529 - acc: 0.968 - ETA: 2s - loss: 0.1529 - acc: 0.968 - ETA: 1s - loss: 0.1529 - acc: 0.968 - ETA: 1s - loss: 0.1528 - acc: 0.968 - ETA: 1s - loss: 0.1528 - acc: 0.968 - ETA: 1s - loss: 0.1529 - acc: 0.968 - ETA: 1s - loss: 0.1529 - acc: 0.968 - ETA: 0s - loss: 0.1529 - acc: 0.968 - ETA: 0s - loss: 0.1528 - acc: 0.968 - ETA: 0s - loss: 0.1529 - acc: 0.968 - ETA: 0s - loss: 0.1529 - acc: 0.968 - ETA: 0s - loss: 0.1528 - acc: 0.968 - 17s 46us/step - loss: 0.1528 - acc: 0.9687 - val_loss: 0.3804 - val_acc: 0.8925\n",
      "Epoch 8/10\n",
      "377144/377144 [==============================] - ETA: 14s - loss: 0.1426 - acc: 0.97 - ETA: 14s - loss: 0.1412 - acc: 0.97 - ETA: 14s - loss: 0.1419 - acc: 0.97 - ETA: 13s - loss: 0.1410 - acc: 0.97 - ETA: 13s - loss: 0.1402 - acc: 0.97 - ETA: 13s - loss: 0.1400 - acc: 0.97 - ETA: 13s - loss: 0.1404 - acc: 0.97 - ETA: 13s - loss: 0.1403 - acc: 0.97 - ETA: 12s - loss: 0.1401 - acc: 0.97 - ETA: 12s - loss: 0.1402 - acc: 0.97 - ETA: 12s - loss: 0.1403 - acc: 0.97 - ETA: 12s - loss: 0.1399 - acc: 0.97 - ETA: 12s - loss: 0.1401 - acc: 0.97 - ETA: 12s - loss: 0.1403 - acc: 0.97 - ETA: 11s - loss: 0.1403 - acc: 0.97 - ETA: 11s - loss: 0.1403 - acc: 0.97 - ETA: 11s - loss: 0.1404 - acc: 0.97 - ETA: 11s - loss: 0.1410 - acc: 0.97 - ETA: 11s - loss: 0.1412 - acc: 0.97 - ETA: 10s - loss: 0.1413 - acc: 0.97 - ETA: 10s - loss: 0.1417 - acc: 0.97 - ETA: 10s - loss: 0.1417 - acc: 0.97 - ETA: 10s - loss: 0.1417 - acc: 0.97 - ETA: 10s - loss: 0.1418 - acc: 0.97 - ETA: 10s - loss: 0.1417 - acc: 0.97 - ETA: 9s - loss: 0.1415 - acc: 0.9739 - ETA: 9s - loss: 0.1414 - acc: 0.974 - ETA: 9s - loss: 0.1414 - acc: 0.974 - ETA: 9s - loss: 0.1413 - acc: 0.973 - ETA: 9s - loss: 0.1414 - acc: 0.973 - ETA: 8s - loss: 0.1414 - acc: 0.973 - ETA: 8s - loss: 0.1413 - acc: 0.973 - ETA: 8s - loss: 0.1412 - acc: 0.974 - ETA: 8s - loss: 0.1412 - acc: 0.974 - ETA: 8s - loss: 0.1410 - acc: 0.974 - ETA: 7s - loss: 0.1409 - acc: 0.974 - ETA: 7s - loss: 0.1410 - acc: 0.974 - ETA: 7s - loss: 0.1409 - acc: 0.974 - ETA: 7s - loss: 0.1408 - acc: 0.974 - ETA: 7s - loss: 0.1406 - acc: 0.974 - ETA: 6s - loss: 0.1406 - acc: 0.974 - ETA: 6s - loss: 0.1406 - acc: 0.974 - ETA: 6s - loss: 0.1405 - acc: 0.974 - ETA: 6s - loss: 0.1406 - acc: 0.974 - ETA: 6s - loss: 0.1405 - acc: 0.974 - ETA: 5s - loss: 0.1404 - acc: 0.974 - ETA: 5s - loss: 0.1404 - acc: 0.974 - ETA: 5s - loss: 0.1404 - acc: 0.974 - ETA: 5s - loss: 0.1403 - acc: 0.974 - ETA: 5s - loss: 0.1403 - acc: 0.974 - ETA: 4s - loss: 0.1403 - acc: 0.974 - ETA: 4s - loss: 0.1403 - acc: 0.974 - ETA: 4s - loss: 0.1403 - acc: 0.974 - ETA: 4s - loss: 0.1402 - acc: 0.973 - ETA: 4s - loss: 0.1402 - acc: 0.973 - ETA: 3s - loss: 0.1401 - acc: 0.973 - ETA: 3s - loss: 0.1402 - acc: 0.973 - ETA: 3s - loss: 0.1401 - acc: 0.973 - ETA: 3s - loss: 0.1401 - acc: 0.973 - ETA: 3s - loss: 0.1401 - acc: 0.973 - ETA: 2s - loss: 0.1400 - acc: 0.973 - ETA: 2s - loss: 0.1400 - acc: 0.973 - ETA: 2s - loss: 0.1400 - acc: 0.973 - ETA: 2s - loss: 0.1399 - acc: 0.974 - ETA: 2s - loss: 0.1399 - acc: 0.973 - ETA: 1s - loss: 0.1399 - acc: 0.973 - ETA: 1s - loss: 0.1399 - acc: 0.973 - ETA: 1s - loss: 0.1398 - acc: 0.973 - ETA: 1s - loss: 0.1397 - acc: 0.973 - ETA: 1s - loss: 0.1397 - acc: 0.973 - ETA: 0s - loss: 0.1397 - acc: 0.973 - ETA: 0s - loss: 0.1396 - acc: 0.973 - ETA: 0s - loss: 0.1396 - acc: 0.973 - ETA: 0s - loss: 0.1396 - acc: 0.973 - ETA: 0s - loss: 0.1395 - acc: 0.973 - 17s 46us/step - loss: 0.1395 - acc: 0.9739 - val_loss: 0.3912 - val_acc: 0.8925\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377144/377144 [==============================] - ETA: 14s - loss: 0.1313 - acc: 0.98 - ETA: 14s - loss: 0.1312 - acc: 0.98 - ETA: 14s - loss: 0.1297 - acc: 0.97 - ETA: 14s - loss: 0.1305 - acc: 0.97 - ETA: 14s - loss: 0.1311 - acc: 0.97 - ETA: 13s - loss: 0.1317 - acc: 0.97 - ETA: 13s - loss: 0.1311 - acc: 0.97 - ETA: 13s - loss: 0.1305 - acc: 0.97 - ETA: 13s - loss: 0.1296 - acc: 0.97 - ETA: 13s - loss: 0.1298 - acc: 0.97 - ETA: 12s - loss: 0.1295 - acc: 0.97 - ETA: 12s - loss: 0.1298 - acc: 0.97 - ETA: 12s - loss: 0.1297 - acc: 0.97 - ETA: 12s - loss: 0.1296 - acc: 0.97 - ETA: 12s - loss: 0.1299 - acc: 0.97 - ETA: 11s - loss: 0.1300 - acc: 0.97 - ETA: 11s - loss: 0.1301 - acc: 0.97 - ETA: 11s - loss: 0.1303 - acc: 0.97 - ETA: 11s - loss: 0.1304 - acc: 0.97 - ETA: 11s - loss: 0.1302 - acc: 0.97 - ETA: 11s - loss: 0.1301 - acc: 0.97 - ETA: 10s - loss: 0.1301 - acc: 0.97 - ETA: 10s - loss: 0.1303 - acc: 0.97 - ETA: 10s - loss: 0.1303 - acc: 0.97 - ETA: 10s - loss: 0.1303 - acc: 0.97 - ETA: 10s - loss: 0.1305 - acc: 0.97 - ETA: 9s - loss: 0.1303 - acc: 0.9776 - ETA: 9s - loss: 0.1304 - acc: 0.977 - ETA: 9s - loss: 0.1305 - acc: 0.977 - ETA: 9s - loss: 0.1304 - acc: 0.977 - ETA: 9s - loss: 0.1304 - acc: 0.977 - ETA: 8s - loss: 0.1306 - acc: 0.977 - ETA: 8s - loss: 0.1306 - acc: 0.977 - ETA: 8s - loss: 0.1304 - acc: 0.977 - ETA: 8s - loss: 0.1302 - acc: 0.977 - ETA: 8s - loss: 0.1302 - acc: 0.977 - ETA: 7s - loss: 0.1302 - acc: 0.977 - ETA: 7s - loss: 0.1301 - acc: 0.977 - ETA: 7s - loss: 0.1302 - acc: 0.977 - ETA: 7s - loss: 0.1301 - acc: 0.977 - ETA: 7s - loss: 0.1300 - acc: 0.977 - ETA: 6s - loss: 0.1299 - acc: 0.977 - ETA: 6s - loss: 0.1297 - acc: 0.977 - ETA: 6s - loss: 0.1297 - acc: 0.977 - ETA: 6s - loss: 0.1298 - acc: 0.977 - ETA: 6s - loss: 0.1299 - acc: 0.977 - ETA: 5s - loss: 0.1298 - acc: 0.977 - ETA: 5s - loss: 0.1297 - acc: 0.977 - ETA: 5s - loss: 0.1297 - acc: 0.977 - ETA: 5s - loss: 0.1298 - acc: 0.977 - ETA: 4s - loss: 0.1297 - acc: 0.977 - ETA: 4s - loss: 0.1298 - acc: 0.977 - ETA: 4s - loss: 0.1298 - acc: 0.977 - ETA: 4s - loss: 0.1298 - acc: 0.977 - ETA: 4s - loss: 0.1297 - acc: 0.977 - ETA: 3s - loss: 0.1297 - acc: 0.977 - ETA: 3s - loss: 0.1296 - acc: 0.977 - ETA: 3s - loss: 0.1296 - acc: 0.977 - ETA: 3s - loss: 0.1295 - acc: 0.977 - ETA: 3s - loss: 0.1294 - acc: 0.977 - ETA: 2s - loss: 0.1295 - acc: 0.977 - ETA: 2s - loss: 0.1295 - acc: 0.977 - ETA: 2s - loss: 0.1295 - acc: 0.977 - ETA: 2s - loss: 0.1294 - acc: 0.977 - ETA: 2s - loss: 0.1294 - acc: 0.977 - ETA: 1s - loss: 0.1293 - acc: 0.977 - ETA: 1s - loss: 0.1292 - acc: 0.977 - ETA: 1s - loss: 0.1292 - acc: 0.977 - ETA: 1s - loss: 0.1292 - acc: 0.977 - ETA: 1s - loss: 0.1291 - acc: 0.977 - ETA: 0s - loss: 0.1290 - acc: 0.977 - ETA: 0s - loss: 0.1290 - acc: 0.977 - ETA: 0s - loss: 0.1289 - acc: 0.977 - ETA: 0s - loss: 0.1289 - acc: 0.977 - ETA: 0s - loss: 0.1289 - acc: 0.977 - 17s 46us/step - loss: 0.1289 - acc: 0.9774 - val_loss: 0.4169 - val_acc: 0.8916\n",
      "Epoch 10/10\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.1186 - acc: 0.98 - ETA: 15s - loss: 0.1213 - acc: 0.98 - ETA: 14s - loss: 0.1220 - acc: 0.98 - ETA: 14s - loss: 0.1230 - acc: 0.97 - ETA: 14s - loss: 0.1235 - acc: 0.97 - ETA: 13s - loss: 0.1224 - acc: 0.98 - ETA: 13s - loss: 0.1231 - acc: 0.97 - ETA: 13s - loss: 0.1227 - acc: 0.97 - ETA: 13s - loss: 0.1228 - acc: 0.97 - ETA: 13s - loss: 0.1230 - acc: 0.97 - ETA: 13s - loss: 0.1232 - acc: 0.97 - ETA: 12s - loss: 0.1229 - acc: 0.97 - ETA: 12s - loss: 0.1229 - acc: 0.97 - ETA: 12s - loss: 0.1224 - acc: 0.97 - ETA: 12s - loss: 0.1222 - acc: 0.97 - ETA: 12s - loss: 0.1222 - acc: 0.97 - ETA: 11s - loss: 0.1222 - acc: 0.97 - ETA: 11s - loss: 0.1221 - acc: 0.97 - ETA: 11s - loss: 0.1222 - acc: 0.97 - ETA: 11s - loss: 0.1223 - acc: 0.97 - ETA: 10s - loss: 0.1222 - acc: 0.97 - ETA: 10s - loss: 0.1222 - acc: 0.97 - ETA: 10s - loss: 0.1221 - acc: 0.97 - ETA: 10s - loss: 0.1220 - acc: 0.97 - ETA: 10s - loss: 0.1219 - acc: 0.97 - ETA: 10s - loss: 0.1218 - acc: 0.97 - ETA: 9s - loss: 0.1217 - acc: 0.9797 - ETA: 9s - loss: 0.1217 - acc: 0.979 - ETA: 9s - loss: 0.1215 - acc: 0.979 - ETA: 9s - loss: 0.1214 - acc: 0.980 - ETA: 9s - loss: 0.1214 - acc: 0.980 - ETA: 8s - loss: 0.1213 - acc: 0.979 - ETA: 8s - loss: 0.1212 - acc: 0.979 - ETA: 8s - loss: 0.1213 - acc: 0.979 - ETA: 8s - loss: 0.1213 - acc: 0.979 - ETA: 8s - loss: 0.1212 - acc: 0.979 - ETA: 7s - loss: 0.1212 - acc: 0.979 - ETA: 7s - loss: 0.1212 - acc: 0.979 - ETA: 7s - loss: 0.1212 - acc: 0.979 - ETA: 7s - loss: 0.1211 - acc: 0.979 - ETA: 7s - loss: 0.1211 - acc: 0.979 - ETA: 6s - loss: 0.1209 - acc: 0.979 - ETA: 6s - loss: 0.1210 - acc: 0.979 - ETA: 6s - loss: 0.1210 - acc: 0.979 - ETA: 6s - loss: 0.1210 - acc: 0.979 - ETA: 6s - loss: 0.1210 - acc: 0.979 - ETA: 5s - loss: 0.1210 - acc: 0.979 - ETA: 5s - loss: 0.1210 - acc: 0.979 - ETA: 5s - loss: 0.1211 - acc: 0.979 - ETA: 5s - loss: 0.1211 - acc: 0.979 - ETA: 4s - loss: 0.1210 - acc: 0.979 - ETA: 4s - loss: 0.1210 - acc: 0.979 - ETA: 4s - loss: 0.1210 - acc: 0.979 - ETA: 4s - loss: 0.1211 - acc: 0.979 - ETA: 4s - loss: 0.1211 - acc: 0.979 - ETA: 3s - loss: 0.1211 - acc: 0.979 - ETA: 3s - loss: 0.1210 - acc: 0.979 - ETA: 3s - loss: 0.1210 - acc: 0.979 - ETA: 3s - loss: 0.1209 - acc: 0.979 - ETA: 3s - loss: 0.1209 - acc: 0.979 - ETA: 2s - loss: 0.1208 - acc: 0.979 - ETA: 2s - loss: 0.1208 - acc: 0.979 - ETA: 2s - loss: 0.1207 - acc: 0.979 - ETA: 2s - loss: 0.1208 - acc: 0.979 - ETA: 2s - loss: 0.1208 - acc: 0.979 - ETA: 1s - loss: 0.1207 - acc: 0.979 - ETA: 1s - loss: 0.1207 - acc: 0.979 - ETA: 1s - loss: 0.1206 - acc: 0.979 - ETA: 1s - loss: 0.1206 - acc: 0.979 - ETA: 1s - loss: 0.1205 - acc: 0.979 - ETA: 0s - loss: 0.1205 - acc: 0.979 - ETA: 0s - loss: 0.1205 - acc: 0.979 - ETA: 0s - loss: 0.1205 - acc: 0.979 - ETA: 0s - loss: 0.1204 - acc: 0.979 - ETA: 0s - loss: 0.1204 - acc: 0.979 - 17s 46us/step - loss: 0.1204 - acc: 0.9797 - val_loss: 0.4507 - val_acc: 0.8914\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=5000,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy validate: 89.136403\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=2)\n",
    "print('Accuracy validate: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "vect_range=list(range(30))\n",
    "range_epoch = pandas.DataFrame(vect_range)\n",
    "val_acc2 = pandas.DataFrame(val_acc)\n",
    "best_acc = pandas.concat([range_epoch, val_acc2], axis=1)\n",
    "best_acc.columns = ['a','b']\n",
    "epoch=best_acc.loc[best_acc['b']==max(best_acc['b']),\"a\"]\n",
    "\n",
    "new_epoch=int(epoch+1)\n",
    "print(int(new_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 300)          27000300  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 45000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1440032   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 28,440,365\n",
      "Trainable params: 28,440,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words+1, embed_size, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable = False\n",
    "\n",
    "#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 377144 samples, validate on 185758 samples\n",
      "Epoch 1/4\n",
      "377144/377144 [==============================] - ETA: 1:55 - loss: 0.6651 - acc: 0.701 - ETA: 1:04 - loss: 0.6659 - acc: 0.732 - ETA: 47s - loss: 0.6595 - acc: 0.740 - ETA: 39s - loss: 0.6304 - acc: 0.74 - ETA: 34s - loss: 0.6110 - acc: 0.75 - ETA: 30s - loss: 0.5980 - acc: 0.75 - ETA: 27s - loss: 0.5854 - acc: 0.75 - ETA: 25s - loss: 0.5732 - acc: 0.75 - ETA: 24s - loss: 0.5622 - acc: 0.76 - ETA: 23s - loss: 0.5568 - acc: 0.76 - ETA: 22s - loss: 0.5487 - acc: 0.76 - ETA: 21s - loss: 0.5408 - acc: 0.76 - ETA: 20s - loss: 0.5335 - acc: 0.76 - ETA: 19s - loss: 0.5274 - acc: 0.76 - ETA: 18s - loss: 0.5215 - acc: 0.76 - ETA: 18s - loss: 0.5157 - acc: 0.76 - ETA: 17s - loss: 0.5110 - acc: 0.76 - ETA: 17s - loss: 0.5050 - acc: 0.76 - ETA: 16s - loss: 0.5011 - acc: 0.76 - ETA: 16s - loss: 0.4965 - acc: 0.76 - ETA: 15s - loss: 0.4930 - acc: 0.76 - ETA: 15s - loss: 0.4893 - acc: 0.76 - ETA: 15s - loss: 0.4860 - acc: 0.76 - ETA: 14s - loss: 0.4827 - acc: 0.76 - ETA: 14s - loss: 0.4800 - acc: 0.76 - ETA: 14s - loss: 0.4765 - acc: 0.76 - ETA: 13s - loss: 0.4743 - acc: 0.76 - ETA: 13s - loss: 0.4715 - acc: 0.76 - ETA: 13s - loss: 0.4690 - acc: 0.76 - ETA: 12s - loss: 0.4662 - acc: 0.76 - ETA: 12s - loss: 0.4633 - acc: 0.76 - ETA: 12s - loss: 0.4611 - acc: 0.76 - ETA: 12s - loss: 0.4586 - acc: 0.76 - ETA: 11s - loss: 0.4561 - acc: 0.76 - ETA: 11s - loss: 0.4542 - acc: 0.76 - ETA: 11s - loss: 0.4518 - acc: 0.76 - ETA: 10s - loss: 0.4500 - acc: 0.76 - ETA: 10s - loss: 0.4479 - acc: 0.76 - ETA: 10s - loss: 0.4459 - acc: 0.77 - ETA: 10s - loss: 0.4445 - acc: 0.77 - ETA: 10s - loss: 0.4430 - acc: 0.77 - ETA: 9s - loss: 0.4412 - acc: 0.7746 - ETA: 9s - loss: 0.4396 - acc: 0.775 - ETA: 9s - loss: 0.4379 - acc: 0.777 - ETA: 9s - loss: 0.4362 - acc: 0.778 - ETA: 8s - loss: 0.4346 - acc: 0.780 - ETA: 8s - loss: 0.4333 - acc: 0.781 - ETA: 8s - loss: 0.4317 - acc: 0.782 - ETA: 8s - loss: 0.4302 - acc: 0.783 - ETA: 8s - loss: 0.4288 - acc: 0.784 - ETA: 7s - loss: 0.4274 - acc: 0.785 - ETA: 7s - loss: 0.4260 - acc: 0.786 - ETA: 7s - loss: 0.4245 - acc: 0.788 - ETA: 7s - loss: 0.4235 - acc: 0.788 - ETA: 7s - loss: 0.4220 - acc: 0.790 - ETA: 6s - loss: 0.4209 - acc: 0.791 - ETA: 6s - loss: 0.4194 - acc: 0.792 - ETA: 6s - loss: 0.4182 - acc: 0.793 - ETA: 6s - loss: 0.4168 - acc: 0.794 - ETA: 6s - loss: 0.4158 - acc: 0.794 - ETA: 5s - loss: 0.4149 - acc: 0.795 - ETA: 5s - loss: 0.4140 - acc: 0.796 - ETA: 5s - loss: 0.4132 - acc: 0.797 - ETA: 5s - loss: 0.4121 - acc: 0.798 - ETA: 5s - loss: 0.4111 - acc: 0.799 - ETA: 4s - loss: 0.4100 - acc: 0.800 - ETA: 4s - loss: 0.4092 - acc: 0.800 - ETA: 4s - loss: 0.4083 - acc: 0.801 - ETA: 4s - loss: 0.4072 - acc: 0.802 - ETA: 4s - loss: 0.4065 - acc: 0.803 - ETA: 3s - loss: 0.4055 - acc: 0.803 - ETA: 3s - loss: 0.4046 - acc: 0.804 - ETA: 3s - loss: 0.4037 - acc: 0.805 - ETA: 3s - loss: 0.4031 - acc: 0.805 - ETA: 3s - loss: 0.4023 - acc: 0.806 - ETA: 2s - loss: 0.4015 - acc: 0.807 - ETA: 2s - loss: 0.4005 - acc: 0.808 - ETA: 2s - loss: 0.3997 - acc: 0.808 - ETA: 2s - loss: 0.3990 - acc: 0.809 - ETA: 2s - loss: 0.3984 - acc: 0.809 - ETA: 2s - loss: 0.3977 - acc: 0.810 - ETA: 1s - loss: 0.3970 - acc: 0.811 - ETA: 1s - loss: 0.3963 - acc: 0.811 - ETA: 1s - loss: 0.3956 - acc: 0.812 - ETA: 1s - loss: 0.3949 - acc: 0.813 - ETA: 1s - loss: 0.3942 - acc: 0.813 - ETA: 0s - loss: 0.3935 - acc: 0.814 - ETA: 0s - loss: 0.3927 - acc: 0.814 - ETA: 0s - loss: 0.3920 - acc: 0.815 - ETA: 0s - loss: 0.3914 - acc: 0.815 - ETA: 0s - loss: 0.3906 - acc: 0.816 - ETA: 0s - loss: 0.3898 - acc: 0.817 - 19s 50us/step - loss: 0.3898 - acc: 0.8172 - val_loss: 0.3197 - val_acc: 0.8731\n",
      "Epoch 2/4\n",
      "377144/377144 [==============================] - ETA: 14s - loss: 0.2936 - acc: 0.88 - ETA: 15s - loss: 0.2859 - acc: 0.88 - ETA: 14s - loss: 0.2840 - acc: 0.89 - ETA: 14s - loss: 0.2822 - acc: 0.88 - ETA: 14s - loss: 0.2801 - acc: 0.89 - ETA: 14s - loss: 0.2768 - acc: 0.89 - ETA: 14s - loss: 0.2748 - acc: 0.89 - ETA: 13s - loss: 0.2739 - acc: 0.89 - ETA: 13s - loss: 0.2732 - acc: 0.89 - ETA: 13s - loss: 0.2717 - acc: 0.89 - ETA: 13s - loss: 0.2707 - acc: 0.89 - ETA: 13s - loss: 0.2699 - acc: 0.89 - ETA: 13s - loss: 0.2691 - acc: 0.89 - ETA: 13s - loss: 0.2673 - acc: 0.89 - ETA: 12s - loss: 0.2660 - acc: 0.89 - ETA: 12s - loss: 0.2658 - acc: 0.89 - ETA: 12s - loss: 0.2650 - acc: 0.89 - ETA: 12s - loss: 0.2638 - acc: 0.89 - ETA: 12s - loss: 0.2626 - acc: 0.89 - ETA: 12s - loss: 0.2625 - acc: 0.89 - ETA: 11s - loss: 0.2612 - acc: 0.89 - ETA: 11s - loss: 0.2606 - acc: 0.89 - ETA: 11s - loss: 0.2600 - acc: 0.89 - ETA: 11s - loss: 0.2598 - acc: 0.89 - ETA: 11s - loss: 0.2594 - acc: 0.89 - ETA: 10s - loss: 0.2585 - acc: 0.89 - ETA: 10s - loss: 0.2583 - acc: 0.89 - ETA: 10s - loss: 0.2574 - acc: 0.89 - ETA: 10s - loss: 0.2566 - acc: 0.89 - ETA: 10s - loss: 0.2559 - acc: 0.89 - ETA: 10s - loss: 0.2554 - acc: 0.89 - ETA: 10s - loss: 0.2548 - acc: 0.89 - ETA: 9s - loss: 0.2542 - acc: 0.8949 - ETA: 9s - loss: 0.2537 - acc: 0.895 - ETA: 9s - loss: 0.2534 - acc: 0.895 - ETA: 9s - loss: 0.2529 - acc: 0.895 - ETA: 9s - loss: 0.2525 - acc: 0.895 - ETA: 8s - loss: 0.2518 - acc: 0.895 - ETA: 8s - loss: 0.2517 - acc: 0.895 - ETA: 8s - loss: 0.2509 - acc: 0.896 - ETA: 8s - loss: 0.2501 - acc: 0.896 - ETA: 8s - loss: 0.2496 - acc: 0.896 - ETA: 8s - loss: 0.2493 - acc: 0.896 - ETA: 8s - loss: 0.2494 - acc: 0.896 - ETA: 7s - loss: 0.2490 - acc: 0.896 - ETA: 7s - loss: 0.2486 - acc: 0.896 - ETA: 7s - loss: 0.2483 - acc: 0.897 - ETA: 7s - loss: 0.2481 - acc: 0.897 - ETA: 7s - loss: 0.2478 - acc: 0.897 - ETA: 7s - loss: 0.2473 - acc: 0.897 - ETA: 6s - loss: 0.2471 - acc: 0.897 - ETA: 6s - loss: 0.2467 - acc: 0.897 - ETA: 6s - loss: 0.2463 - acc: 0.897 - ETA: 6s - loss: 0.2458 - acc: 0.897 - ETA: 6s - loss: 0.2457 - acc: 0.897 - ETA: 5s - loss: 0.2452 - acc: 0.898 - ETA: 5s - loss: 0.2445 - acc: 0.898 - ETA: 5s - loss: 0.2444 - acc: 0.898 - ETA: 5s - loss: 0.2440 - acc: 0.898 - ETA: 5s - loss: 0.2435 - acc: 0.898 - ETA: 5s - loss: 0.2432 - acc: 0.898 - ETA: 4s - loss: 0.2429 - acc: 0.899 - ETA: 4s - loss: 0.2429 - acc: 0.898 - ETA: 4s - loss: 0.2430 - acc: 0.898 - ETA: 4s - loss: 0.2427 - acc: 0.899 - ETA: 4s - loss: 0.2423 - acc: 0.899 - ETA: 4s - loss: 0.2420 - acc: 0.899 - ETA: 3s - loss: 0.2418 - acc: 0.899 - ETA: 3s - loss: 0.2416 - acc: 0.899 - ETA: 3s - loss: 0.2413 - acc: 0.899 - ETA: 3s - loss: 0.2411 - acc: 0.899 - ETA: 3s - loss: 0.2407 - acc: 0.899 - ETA: 3s - loss: 0.2405 - acc: 0.899 - ETA: 2s - loss: 0.2403 - acc: 0.899 - ETA: 2s - loss: 0.2402 - acc: 0.899 - ETA: 2s - loss: 0.2400 - acc: 0.900 - ETA: 2s - loss: 0.2398 - acc: 0.900 - ETA: 2s - loss: 0.2398 - acc: 0.900 - ETA: 2s - loss: 0.2394 - acc: 0.900 - ETA: 1s - loss: 0.2392 - acc: 0.900 - ETA: 1s - loss: 0.2390 - acc: 0.900 - ETA: 1s - loss: 0.2388 - acc: 0.900 - ETA: 1s - loss: 0.2388 - acc: 0.900 - ETA: 1s - loss: 0.2385 - acc: 0.900 - ETA: 1s - loss: 0.2383 - acc: 0.900 - ETA: 1s - loss: 0.2382 - acc: 0.900 - ETA: 0s - loss: 0.2380 - acc: 0.900 - ETA: 0s - loss: 0.2379 - acc: 0.901 - ETA: 0s - loss: 0.2377 - acc: 0.901 - ETA: 0s - loss: 0.2376 - acc: 0.901 - ETA: 0s - loss: 0.2375 - acc: 0.901 - ETA: 0s - loss: 0.2374 - acc: 0.901 - 17s 46us/step - loss: 0.2374 - acc: 0.9012 - val_loss: 0.2521 - val_acc: 0.8952\n",
      "Epoch 3/4\n",
      "377144/377144 [==============================] - ETA: 15s - loss: 0.1744 - acc: 0.93 - ETA: 15s - loss: 0.1708 - acc: 0.93 - ETA: 14s - loss: 0.1697 - acc: 0.93 - ETA: 14s - loss: 0.1701 - acc: 0.93 - ETA: 14s - loss: 0.1679 - acc: 0.93 - ETA: 14s - loss: 0.1673 - acc: 0.93 - ETA: 14s - loss: 0.1661 - acc: 0.93 - ETA: 13s - loss: 0.1643 - acc: 0.93 - ETA: 13s - loss: 0.1642 - acc: 0.93 - ETA: 13s - loss: 0.1642 - acc: 0.93 - ETA: 13s - loss: 0.1644 - acc: 0.93 - ETA: 13s - loss: 0.1655 - acc: 0.93 - ETA: 13s - loss: 0.1654 - acc: 0.93 - ETA: 12s - loss: 0.1652 - acc: 0.93 - ETA: 12s - loss: 0.1654 - acc: 0.93 - ETA: 12s - loss: 0.1653 - acc: 0.93 - ETA: 12s - loss: 0.1649 - acc: 0.93 - ETA: 12s - loss: 0.1647 - acc: 0.93 - ETA: 12s - loss: 0.1645 - acc: 0.93 - ETA: 11s - loss: 0.1649 - acc: 0.93 - ETA: 11s - loss: 0.1645 - acc: 0.93 - ETA: 11s - loss: 0.1655 - acc: 0.93 - ETA: 11s - loss: 0.1652 - acc: 0.93 - ETA: 11s - loss: 0.1654 - acc: 0.93 - ETA: 11s - loss: 0.1653 - acc: 0.93 - ETA: 10s - loss: 0.1649 - acc: 0.93 - ETA: 10s - loss: 0.1648 - acc: 0.93 - ETA: 10s - loss: 0.1652 - acc: 0.93 - ETA: 10s - loss: 0.1652 - acc: 0.93 - ETA: 10s - loss: 0.1653 - acc: 0.93 - ETA: 10s - loss: 0.1652 - acc: 0.93 - ETA: 9s - loss: 0.1652 - acc: 0.9347 - ETA: 9s - loss: 0.1653 - acc: 0.934 - ETA: 9s - loss: 0.1657 - acc: 0.934 - ETA: 9s - loss: 0.1656 - acc: 0.934 - ETA: 9s - loss: 0.1654 - acc: 0.934 - ETA: 9s - loss: 0.1654 - acc: 0.934 - ETA: 8s - loss: 0.1657 - acc: 0.934 - ETA: 8s - loss: 0.1656 - acc: 0.934 - ETA: 8s - loss: 0.1658 - acc: 0.934 - ETA: 8s - loss: 0.1659 - acc: 0.934 - ETA: 8s - loss: 0.1659 - acc: 0.934 - ETA: 8s - loss: 0.1658 - acc: 0.934 - ETA: 7s - loss: 0.1656 - acc: 0.934 - ETA: 7s - loss: 0.1658 - acc: 0.934 - ETA: 7s - loss: 0.1657 - acc: 0.934 - ETA: 7s - loss: 0.1658 - acc: 0.934 - ETA: 7s - loss: 0.1660 - acc: 0.934 - ETA: 7s - loss: 0.1664 - acc: 0.934 - ETA: 6s - loss: 0.1665 - acc: 0.933 - ETA: 6s - loss: 0.1664 - acc: 0.933 - ETA: 6s - loss: 0.1664 - acc: 0.933 - ETA: 6s - loss: 0.1665 - acc: 0.933 - ETA: 6s - loss: 0.1666 - acc: 0.933 - ETA: 6s - loss: 0.1667 - acc: 0.933 - ETA: 6s - loss: 0.1668 - acc: 0.933 - ETA: 5s - loss: 0.1668 - acc: 0.933 - ETA: 5s - loss: 0.1669 - acc: 0.933 - ETA: 5s - loss: 0.1671 - acc: 0.933 - ETA: 5s - loss: 0.1672 - acc: 0.933 - ETA: 5s - loss: 0.1674 - acc: 0.933 - ETA: 5s - loss: 0.1673 - acc: 0.933 - ETA: 4s - loss: 0.1674 - acc: 0.933 - ETA: 4s - loss: 0.1674 - acc: 0.933 - ETA: 4s - loss: 0.1673 - acc: 0.933 - ETA: 4s - loss: 0.1672 - acc: 0.933 - ETA: 4s - loss: 0.1670 - acc: 0.933 - ETA: 4s - loss: 0.1671 - acc: 0.933 - ETA: 3s - loss: 0.1670 - acc: 0.933 - ETA: 3s - loss: 0.1669 - acc: 0.933 - ETA: 3s - loss: 0.1668 - acc: 0.933 - ETA: 3s - loss: 0.1667 - acc: 0.933 - ETA: 3s - loss: 0.1668 - acc: 0.933 - ETA: 3s - loss: 0.1668 - acc: 0.933 - ETA: 2s - loss: 0.1669 - acc: 0.933 - ETA: 2s - loss: 0.1668 - acc: 0.933 - ETA: 2s - loss: 0.1667 - acc: 0.933 - ETA: 2s - loss: 0.1666 - acc: 0.933 - ETA: 2s - loss: 0.1664 - acc: 0.933 - ETA: 2s - loss: 0.1665 - acc: 0.933 - ETA: 1s - loss: 0.1666 - acc: 0.933 - ETA: 1s - loss: 0.1667 - acc: 0.933 - ETA: 1s - loss: 0.1668 - acc: 0.933 - ETA: 1s - loss: 0.1669 - acc: 0.933 - ETA: 1s - loss: 0.1669 - acc: 0.933 - ETA: 1s - loss: 0.1669 - acc: 0.933 - ETA: 0s - loss: 0.1670 - acc: 0.933 - ETA: 0s - loss: 0.1670 - acc: 0.933 - ETA: 0s - loss: 0.1671 - acc: 0.933 - ETA: 0s - loss: 0.1671 - acc: 0.933 - ETA: 0s - loss: 0.1671 - acc: 0.933 - ETA: 0s - loss: 0.1672 - acc: 0.933 - 17s 46us/step - loss: 0.1672 - acc: 0.9331 - val_loss: 0.2610 - val_acc: 0.8940\n",
      "Epoch 4/4\n",
      "377144/377144 [==============================] - ETA: 14s - loss: 0.1177 - acc: 0.96 - ETA: 14s - loss: 0.1209 - acc: 0.95 - ETA: 14s - loss: 0.1206 - acc: 0.95 - ETA: 14s - loss: 0.1200 - acc: 0.95 - ETA: 14s - loss: 0.1197 - acc: 0.95 - ETA: 14s - loss: 0.1202 - acc: 0.95 - ETA: 14s - loss: 0.1196 - acc: 0.95 - ETA: 13s - loss: 0.1177 - acc: 0.95 - ETA: 13s - loss: 0.1172 - acc: 0.95 - ETA: 13s - loss: 0.1169 - acc: 0.95 - ETA: 13s - loss: 0.1167 - acc: 0.95 - ETA: 13s - loss: 0.1167 - acc: 0.95 - ETA: 13s - loss: 0.1166 - acc: 0.95 - ETA: 12s - loss: 0.1160 - acc: 0.95 - ETA: 12s - loss: 0.1166 - acc: 0.95 - ETA: 12s - loss: 0.1164 - acc: 0.95 - ETA: 12s - loss: 0.1172 - acc: 0.95 - ETA: 12s - loss: 0.1169 - acc: 0.95 - ETA: 12s - loss: 0.1171 - acc: 0.95 - ETA: 11s - loss: 0.1171 - acc: 0.95 - ETA: 11s - loss: 0.1171 - acc: 0.95 - ETA: 11s - loss: 0.1172 - acc: 0.95 - ETA: 11s - loss: 0.1171 - acc: 0.95 - ETA: 11s - loss: 0.1174 - acc: 0.95 - ETA: 11s - loss: 0.1176 - acc: 0.95 - ETA: 11s - loss: 0.1177 - acc: 0.95 - ETA: 10s - loss: 0.1180 - acc: 0.95 - ETA: 10s - loss: 0.1182 - acc: 0.95 - ETA: 10s - loss: 0.1182 - acc: 0.95 - ETA: 10s - loss: 0.1182 - acc: 0.95 - ETA: 10s - loss: 0.1185 - acc: 0.95 - ETA: 9s - loss: 0.1187 - acc: 0.9569 - ETA: 9s - loss: 0.1189 - acc: 0.956 - ETA: 9s - loss: 0.1192 - acc: 0.956 - ETA: 9s - loss: 0.1193 - acc: 0.956 - ETA: 9s - loss: 0.1191 - acc: 0.956 - ETA: 9s - loss: 0.1192 - acc: 0.956 - ETA: 8s - loss: 0.1190 - acc: 0.956 - ETA: 8s - loss: 0.1192 - acc: 0.956 - ETA: 8s - loss: 0.1192 - acc: 0.956 - ETA: 8s - loss: 0.1192 - acc: 0.956 - ETA: 8s - loss: 0.1193 - acc: 0.956 - ETA: 8s - loss: 0.1192 - acc: 0.956 - ETA: 7s - loss: 0.1193 - acc: 0.956 - ETA: 7s - loss: 0.1192 - acc: 0.956 - ETA: 7s - loss: 0.1191 - acc: 0.956 - ETA: 7s - loss: 0.1192 - acc: 0.956 - ETA: 7s - loss: 0.1193 - acc: 0.956 - ETA: 7s - loss: 0.1191 - acc: 0.956 - ETA: 6s - loss: 0.1190 - acc: 0.956 - ETA: 6s - loss: 0.1189 - acc: 0.956 - ETA: 6s - loss: 0.1189 - acc: 0.956 - ETA: 6s - loss: 0.1189 - acc: 0.956 - ETA: 6s - loss: 0.1190 - acc: 0.956 - ETA: 6s - loss: 0.1191 - acc: 0.956 - ETA: 6s - loss: 0.1191 - acc: 0.956 - ETA: 5s - loss: 0.1190 - acc: 0.956 - ETA: 5s - loss: 0.1190 - acc: 0.956 - ETA: 5s - loss: 0.1191 - acc: 0.956 - ETA: 5s - loss: 0.1193 - acc: 0.956 - ETA: 5s - loss: 0.1193 - acc: 0.956 - ETA: 4s - loss: 0.1194 - acc: 0.956 - ETA: 4s - loss: 0.1194 - acc: 0.956 - ETA: 4s - loss: 0.1194 - acc: 0.956 - ETA: 4s - loss: 0.1195 - acc: 0.956 - ETA: 4s - loss: 0.1194 - acc: 0.956 - ETA: 4s - loss: 0.1195 - acc: 0.956 - ETA: 4s - loss: 0.1197 - acc: 0.956 - ETA: 3s - loss: 0.1198 - acc: 0.955 - ETA: 3s - loss: 0.1198 - acc: 0.955 - ETA: 3s - loss: 0.1199 - acc: 0.955 - ETA: 3s - loss: 0.1200 - acc: 0.955 - ETA: 3s - loss: 0.1200 - acc: 0.955 - ETA: 3s - loss: 0.1200 - acc: 0.955 - ETA: 2s - loss: 0.1201 - acc: 0.955 - ETA: 2s - loss: 0.1201 - acc: 0.955 - ETA: 2s - loss: 0.1200 - acc: 0.955 - ETA: 2s - loss: 0.1200 - acc: 0.955 - ETA: 2s - loss: 0.1200 - acc: 0.955 - ETA: 2s - loss: 0.1201 - acc: 0.955 - ETA: 1s - loss: 0.1200 - acc: 0.955 - ETA: 1s - loss: 0.1199 - acc: 0.955 - ETA: 1s - loss: 0.1198 - acc: 0.955 - ETA: 1s - loss: 0.1199 - acc: 0.955 - ETA: 1s - loss: 0.1200 - acc: 0.955 - ETA: 1s - loss: 0.1200 - acc: 0.955 - ETA: 0s - loss: 0.1200 - acc: 0.955 - ETA: 0s - loss: 0.1200 - acc: 0.955 - ETA: 0s - loss: 0.1201 - acc: 0.955 - ETA: 0s - loss: 0.1201 - acc: 0.955 - ETA: 0s - loss: 0.1200 - acc: 0.955 - ETA: 0s - loss: 0.1200 - acc: 0.955 - 17s 46us/step - loss: 0.1200 - acc: 0.9557 - val_loss: 0.2916 - val_acc: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ae4d84d68>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    epochs=new_epoch,\n",
    "                    batch_size=4096,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 97.660840\n",
      "Accuracy validate: 89.339356\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=2)\n",
    "print('Accuracy train: %f' % (accuracy*100))\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=2)\n",
    "print('Accuracy validate: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 89.497162\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Accuracy test: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_val, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_val, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.893394\n",
      "Precision: 0.778497\n",
      "Recall: 0.752000\n",
      "F1 score: 0.765019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_val, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_val, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_val, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_val, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_train, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_train, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.976608\n",
      "Precision: 0.940969\n",
      "Recall: 0.958786\n",
      "F1 score: 0.949794\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_train, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_train, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_train, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_train, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.894972\n",
      "Precision: 0.779659\n",
      "Recall: 0.759509\n",
      "F1 score: 0.769452\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
